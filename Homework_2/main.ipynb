{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time Machine Learning\n",
    "##### Armin Danesh | 801200833\n",
    "##### Github Link: https://github.com/adaneshp/Real-Time-Machine-Learning\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of         price  area  bedrooms  bathrooms  stories mainroad guestroom basement  \\\n",
       "0    13300000  7420         4          2        3      yes        no       no   \n",
       "1    12250000  8960         4          4        4      yes        no       no   \n",
       "2    12250000  9960         3          2        2      yes        no      yes   \n",
       "3    12215000  7500         4          2        2      yes        no      yes   \n",
       "4    11410000  7420         4          1        2      yes       yes      yes   \n",
       "..        ...   ...       ...        ...      ...      ...       ...      ...   \n",
       "540   1820000  3000         2          1        1      yes        no      yes   \n",
       "541   1767150  2400         3          1        1       no        no       no   \n",
       "542   1750000  3620         2          1        1      yes        no       no   \n",
       "543   1750000  2910         3          1        1       no        no       no   \n",
       "544   1750000  3850         3          1        2      yes        no       no   \n",
       "\n",
       "    hotwaterheating airconditioning  parking prefarea furnishingstatus  \n",
       "0                no             yes        2      yes        furnished  \n",
       "1                no             yes        3       no        furnished  \n",
       "2                no              no        2      yes   semi-furnished  \n",
       "3                no             yes        3      yes        furnished  \n",
       "4                no             yes        2       no        furnished  \n",
       "..              ...             ...      ...      ...              ...  \n",
       "540              no              no        2       no      unfurnished  \n",
       "541              no              no        0       no   semi-furnished  \n",
       "542              no              no        0       no      unfurnished  \n",
       "543              no              no        0       no        furnished  \n",
       "544              no              no        0       no      unfurnished  \n",
       "\n",
       "[545 rows x 13 columns]>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = pd.DataFrame(pd.read_csv(\"Housing.csv\"))\n",
    "housing.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((436, 13), (109, 13))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "df_train, df_test = train_test_split(housing, train_size = 0.8, test_size = 0.2, random_state=None)\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>stories</th>\n",
       "      <th>parking</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>3620</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>4000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2695000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>3040</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>3600</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>9860</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4515000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     area  bedrooms  bathrooms  stories  parking    price\n",
       "542  3620         2          1        1        0  1750000\n",
       "496  4000         2          1        1        0  2695000\n",
       "484  3040         2          1        1        0  2870000\n",
       "507  3600         2          1        1        0  2590000\n",
       "252  9860         3          1        1        0  4515000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking','price']\n",
    "df_Newtrain = df_train[num_vars]\n",
    "df_Newtest = df_test[num_vars]\n",
    "df_Newtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4763/818169181.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_Newtrain[num_vars] = scaler.fit_transform(df_Newtrain[num_vars])\n",
      "/tmp/ipykernel_4763/818169181.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_Newtest[num_vars] = scaler.fit_transform(df_Newtest[num_vars])\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df_Newtrain[num_vars] = scaler.fit_transform(df_Newtrain[num_vars])\n",
    "df_Newtest[num_vars] = scaler.fit_transform(df_Newtest[num_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(df_Newtrain[['area', 'bedrooms', 'bathrooms', 'stories', 'parking']].values, dtype = torch.float32)\n",
    "y_train = torch.tensor(df_Newtrain[['price']].values, dtype = torch.float32)\n",
    "x_test = torch.tensor(df_Newtest[['area', 'bedrooms', 'bathrooms', 'stories', 'parking']].values, dtype = torch.float32)\n",
    "y_test = torch.tensor(df_Newtest[['price']].values, dtype = torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net (nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = nn.Linear(n_inputs, 8)\n",
    "        self.predict = nn.Linear(8, n_outputs)\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.predict(x)\n",
    "        x = F.tanh(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 --> train_loss = 0.03600039705634117, val_loss = 0.03594562038779259\n",
      "Epoch #1 --> train_loss = 0.03594562038779259, val_loss = 0.035891093313694\n",
      "Epoch #2 --> train_loss = 0.035891093313694, val_loss = 0.035836830735206604\n",
      "Epoch #3 --> train_loss = 0.035836830735206604, val_loss = 0.0357828326523304\n",
      "Epoch #4 --> train_loss = 0.0357828326523304, val_loss = 0.03572908788919449\n",
      "Epoch #5 --> train_loss = 0.03572908788919449, val_loss = 0.03567560017108917\n",
      "Epoch #6 --> train_loss = 0.03567560017108917, val_loss = 0.03562236949801445\n",
      "Epoch #7 --> train_loss = 0.03562236949801445, val_loss = 0.035569388419389725\n",
      "Epoch #8 --> train_loss = 0.035569388419389725, val_loss = 0.0355166532099247\n",
      "Epoch #9 --> train_loss = 0.0355166532099247, val_loss = 0.035464175045490265\n",
      "Epoch #10 --> train_loss = 0.035464175045490265, val_loss = 0.03541195020079613\n",
      "Epoch #11 --> train_loss = 0.03541195020079613, val_loss = 0.03535997495055199\n",
      "Epoch #12 --> train_loss = 0.03535997495055199, val_loss = 0.03530824929475784\n",
      "Epoch #13 --> train_loss = 0.03530824929475784, val_loss = 0.0352567657828331\n",
      "Epoch #14 --> train_loss = 0.0352567657828331, val_loss = 0.03520553559064865\n",
      "Epoch #15 --> train_loss = 0.03520553559064865, val_loss = 0.0351545475423336\n",
      "Epoch #16 --> train_loss = 0.0351545475423336, val_loss = 0.035103801637887955\n",
      "Epoch #17 --> train_loss = 0.035103801637887955, val_loss = 0.035053301602602005\n",
      "Epoch #18 --> train_loss = 0.035053301602602005, val_loss = 0.035003043711185455\n",
      "Epoch #19 --> train_loss = 0.035003043711185455, val_loss = 0.03495302423834801\n",
      "Epoch #20 --> train_loss = 0.03495302423834801, val_loss = 0.034903254359960556\n",
      "Epoch #21 --> train_loss = 0.034903254359960556, val_loss = 0.03485371544957161\n",
      "Epoch #22 --> train_loss = 0.03485371544957161, val_loss = 0.034804414957761765\n",
      "Epoch #23 --> train_loss = 0.034804414957761765, val_loss = 0.03475535660982132\n",
      "Epoch #24 --> train_loss = 0.03475535660982132, val_loss = 0.03470653295516968\n",
      "Epoch #25 --> train_loss = 0.03470653295516968, val_loss = 0.03465794026851654\n",
      "Epoch #26 --> train_loss = 0.03465794026851654, val_loss = 0.034609586000442505\n",
      "Epoch #27 --> train_loss = 0.034609586000442505, val_loss = 0.034561462700366974\n",
      "Epoch #28 --> train_loss = 0.034561462700366974, val_loss = 0.03451357036828995\n",
      "Epoch #29 --> train_loss = 0.03451357036828995, val_loss = 0.034465912729501724\n",
      "Epoch #30 --> train_loss = 0.034465912729501724, val_loss = 0.03441848233342171\n",
      "Epoch #31 --> train_loss = 0.03441848233342171, val_loss = 0.034371279180049896\n",
      "Epoch #32 --> train_loss = 0.034371279180049896, val_loss = 0.03432431071996689\n",
      "Epoch #33 --> train_loss = 0.03432431071996689, val_loss = 0.03427756950259209\n",
      "Epoch #34 --> train_loss = 0.03427756950259209, val_loss = 0.034231044352054596\n",
      "Epoch #35 --> train_loss = 0.034231044352054596, val_loss = 0.03418475389480591\n",
      "Epoch #36 --> train_loss = 0.03418475389480591, val_loss = 0.03413868695497513\n",
      "Epoch #37 --> train_loss = 0.03413868695497513, val_loss = 0.034092843532562256\n",
      "Epoch #38 --> train_loss = 0.034092843532562256, val_loss = 0.03404721990227699\n",
      "Epoch #39 --> train_loss = 0.03404721990227699, val_loss = 0.03400181233882904\n",
      "Epoch #40 --> train_loss = 0.03400181233882904, val_loss = 0.033956632018089294\n",
      "Epoch #41 --> train_loss = 0.033956632018089294, val_loss = 0.03391167148947716\n",
      "Epoch #42 --> train_loss = 0.03391167148947716, val_loss = 0.03386693447828293\n",
      "Epoch #43 --> train_loss = 0.03386693447828293, val_loss = 0.033822402358055115\n",
      "Epoch #44 --> train_loss = 0.033822402358055115, val_loss = 0.03377809375524521\n",
      "Epoch #45 --> train_loss = 0.03377809375524521, val_loss = 0.033733997493982315\n",
      "Epoch #46 --> train_loss = 0.033733997493982315, val_loss = 0.03369011729955673\n",
      "Epoch #47 --> train_loss = 0.03369011729955673, val_loss = 0.03364644944667816\n",
      "Epoch #48 --> train_loss = 0.03364644944667816, val_loss = 0.0336029939353466\n",
      "Epoch #49 --> train_loss = 0.0336029939353466, val_loss = 0.033559754490852356\n",
      "Epoch #50 --> train_loss = 0.033559754490852356, val_loss = 0.03351672366261482\n",
      "Epoch #51 --> train_loss = 0.03351672366261482, val_loss = 0.0334739051759243\n",
      "Epoch #52 --> train_loss = 0.0334739051759243, val_loss = 0.033431295305490494\n",
      "Epoch #53 --> train_loss = 0.033431295305490494, val_loss = 0.0333888940513134\n",
      "Epoch #54 --> train_loss = 0.0333888940513134, val_loss = 0.033346690237522125\n",
      "Epoch #55 --> train_loss = 0.033346690237522125, val_loss = 0.03330469876527786\n",
      "Epoch #56 --> train_loss = 0.03330469876527786, val_loss = 0.033262915909290314\n",
      "Epoch #57 --> train_loss = 0.033262915909290314, val_loss = 0.03322133794426918\n",
      "Epoch #58 --> train_loss = 0.03322133794426918, val_loss = 0.033179957419633865\n",
      "Epoch #59 --> train_loss = 0.033179957419633865, val_loss = 0.033138781785964966\n",
      "Epoch #60 --> train_loss = 0.033138781785964966, val_loss = 0.033097803592681885\n",
      "Epoch #61 --> train_loss = 0.033097803592681885, val_loss = 0.03305703401565552\n",
      "Epoch #62 --> train_loss = 0.03305703401565552, val_loss = 0.03301646187901497\n",
      "Epoch #63 --> train_loss = 0.03301646187901497, val_loss = 0.03297609090805054\n",
      "Epoch #64 --> train_loss = 0.03297609090805054, val_loss = 0.03293590992689133\n",
      "Epoch #65 --> train_loss = 0.03293590992689133, val_loss = 0.032895926386117935\n",
      "Epoch #66 --> train_loss = 0.032895926386117935, val_loss = 0.03285614401102066\n",
      "Epoch #67 --> train_loss = 0.03285614401102066, val_loss = 0.0328165628015995\n",
      "Epoch #68 --> train_loss = 0.0328165628015995, val_loss = 0.03277716785669327\n",
      "Epoch #69 --> train_loss = 0.03277716785669327, val_loss = 0.03273796662688255\n",
      "Epoch #70 --> train_loss = 0.03273796662688255, val_loss = 0.03269896283745766\n",
      "Epoch #71 --> train_loss = 0.03269896283745766, val_loss = 0.03266014903783798\n",
      "Epoch #72 --> train_loss = 0.03266014903783798, val_loss = 0.03262152522802353\n",
      "Epoch #73 --> train_loss = 0.03262152522802353, val_loss = 0.0325830914080143\n",
      "Epoch #74 --> train_loss = 0.0325830914080143, val_loss = 0.03254484385251999\n",
      "Epoch #75 --> train_loss = 0.03254484385251999, val_loss = 0.0325067900121212\n",
      "Epoch #76 --> train_loss = 0.0325067900121212, val_loss = 0.03246891871094704\n",
      "Epoch #77 --> train_loss = 0.03246891871094704, val_loss = 0.03243124112486839\n",
      "Epoch #78 --> train_loss = 0.03243124112486839, val_loss = 0.032393746078014374\n",
      "Epoch #79 --> train_loss = 0.032393746078014374, val_loss = 0.03235643357038498\n",
      "Epoch #80 --> train_loss = 0.03235643357038498, val_loss = 0.03231930732727051\n",
      "Epoch #81 --> train_loss = 0.03231930732727051, val_loss = 0.03228236734867096\n",
      "Epoch #82 --> train_loss = 0.03228236734867096, val_loss = 0.032245609909296036\n",
      "Epoch #83 --> train_loss = 0.032245609909296036, val_loss = 0.03220903128385544\n",
      "Epoch #84 --> train_loss = 0.03220903128385544, val_loss = 0.032172635197639465\n",
      "Epoch #85 --> train_loss = 0.032172635197639465, val_loss = 0.03213641792535782\n",
      "Epoch #86 --> train_loss = 0.03213641792535782, val_loss = 0.0321003794670105\n",
      "Epoch #87 --> train_loss = 0.0321003794670105, val_loss = 0.032064516097307205\n",
      "Epoch #88 --> train_loss = 0.032064516097307205, val_loss = 0.03202883154153824\n",
      "Epoch #89 --> train_loss = 0.03202883154153824, val_loss = 0.031993329524993896\n",
      "Epoch #90 --> train_loss = 0.031993329524993896, val_loss = 0.031957995146512985\n",
      "Epoch #91 --> train_loss = 0.031957995146512985, val_loss = 0.0319228395819664\n",
      "Epoch #92 --> train_loss = 0.0319228395819664, val_loss = 0.03188786283135414\n",
      "Epoch #93 --> train_loss = 0.03188786283135414, val_loss = 0.03185305744409561\n",
      "Epoch #94 --> train_loss = 0.03185305744409561, val_loss = 0.03181842342019081\n",
      "Epoch #95 --> train_loss = 0.03181842342019081, val_loss = 0.03178395330905914\n",
      "Epoch #96 --> train_loss = 0.03178395330905914, val_loss = 0.0317496657371521\n",
      "Epoch #97 --> train_loss = 0.0317496657371521, val_loss = 0.03171554580330849\n",
      "Epoch #98 --> train_loss = 0.03171554580330849, val_loss = 0.031681593507528305\n",
      "Epoch #99 --> train_loss = 0.031681593507528305, val_loss = 0.031647805124521255\n",
      "Epoch #100 --> train_loss = 0.031647805124521255, val_loss = 0.031614188104867935\n",
      "Epoch #101 --> train_loss = 0.031614188104867935, val_loss = 0.031580742448568344\n",
      "Epoch #102 --> train_loss = 0.031580742448568344, val_loss = 0.031547460705041885\n",
      "Epoch #103 --> train_loss = 0.031547460705041885, val_loss = 0.03151433914899826\n",
      "Epoch #104 --> train_loss = 0.03151433914899826, val_loss = 0.031481388956308365\n",
      "Epoch #105 --> train_loss = 0.031481388956308365, val_loss = 0.0314485989511013\n",
      "Epoch #106 --> train_loss = 0.0314485989511013, val_loss = 0.03141597658395767\n",
      "Epoch #107 --> train_loss = 0.03141597658395767, val_loss = 0.03138351067900658\n",
      "Epoch #108 --> train_loss = 0.03138351067900658, val_loss = 0.03135121241211891\n",
      "Epoch #109 --> train_loss = 0.03135121241211891, val_loss = 0.03131907060742378\n",
      "Epoch #110 --> train_loss = 0.03131907060742378, val_loss = 0.03128708899021149\n",
      "Epoch #111 --> train_loss = 0.03128708899021149, val_loss = 0.031255267560482025\n",
      "Epoch #112 --> train_loss = 0.031255267560482025, val_loss = 0.0312236025929451\n",
      "Epoch #113 --> train_loss = 0.0312236025929451, val_loss = 0.031192097812891006\n",
      "Epoch #114 --> train_loss = 0.031192097812891006, val_loss = 0.031160755082964897\n",
      "Epoch #115 --> train_loss = 0.031160755082964897, val_loss = 0.031129563227295876\n",
      "Epoch #116 --> train_loss = 0.031129563227295876, val_loss = 0.03109852783381939\n",
      "Epoch #117 --> train_loss = 0.03109852783381939, val_loss = 0.031067650765180588\n",
      "Epoch #118 --> train_loss = 0.031067650765180588, val_loss = 0.031036924570798874\n",
      "Epoch #119 --> train_loss = 0.031036924570798874, val_loss = 0.031006351113319397\n",
      "Epoch #120 --> train_loss = 0.031006351113319397, val_loss = 0.030975930392742157\n",
      "Epoch #121 --> train_loss = 0.030975930392742157, val_loss = 0.030945664271712303\n",
      "Epoch #122 --> train_loss = 0.030945664271712303, val_loss = 0.030915549024939537\n",
      "Epoch #123 --> train_loss = 0.030915549024939537, val_loss = 0.03088558465242386\n",
      "Epoch #124 --> train_loss = 0.03088558465242386, val_loss = 0.03085576929152012\n",
      "Epoch #125 --> train_loss = 0.03085576929152012, val_loss = 0.030826102942228317\n",
      "Epoch #126 --> train_loss = 0.030826102942228317, val_loss = 0.030796581879258156\n",
      "Epoch #127 --> train_loss = 0.030796581879258156, val_loss = 0.030767209827899933\n",
      "Epoch #128 --> train_loss = 0.030767209827899933, val_loss = 0.0307379812002182\n",
      "Epoch #129 --> train_loss = 0.0307379812002182, val_loss = 0.030708903446793556\n",
      "Epoch #130 --> train_loss = 0.030708903446793556, val_loss = 0.030679969117045403\n",
      "Epoch #131 --> train_loss = 0.030679969117045403, val_loss = 0.030651181936264038\n",
      "Epoch #132 --> train_loss = 0.030651181936264038, val_loss = 0.030622536316514015\n",
      "Epoch #133 --> train_loss = 0.030622536316514015, val_loss = 0.030594035983085632\n",
      "Epoch #134 --> train_loss = 0.030594035983085632, val_loss = 0.03056567721068859\n",
      "Epoch #135 --> train_loss = 0.03056567721068859, val_loss = 0.030537467449903488\n",
      "Epoch #136 --> train_loss = 0.030537467449903488, val_loss = 0.03050938807427883\n",
      "Epoch #137 --> train_loss = 0.03050938807427883, val_loss = 0.030481461435556412\n",
      "Epoch #138 --> train_loss = 0.030481461435556412, val_loss = 0.030453670769929886\n",
      "Epoch #139 --> train_loss = 0.030453670769929886, val_loss = 0.030426016077399254\n",
      "Epoch #140 --> train_loss = 0.030426016077399254, val_loss = 0.030398504808545113\n",
      "Epoch #141 --> train_loss = 0.030398504808545113, val_loss = 0.030371127650141716\n",
      "Epoch #142 --> train_loss = 0.030371127650141716, val_loss = 0.03034389205276966\n",
      "Epoch #143 --> train_loss = 0.03034389205276966, val_loss = 0.03031679056584835\n",
      "Epoch #144 --> train_loss = 0.03031679056584835, val_loss = 0.030289825052022934\n",
      "Epoch #145 --> train_loss = 0.030289825052022934, val_loss = 0.03026299551129341\n",
      "Epoch #146 --> train_loss = 0.03026299551129341, val_loss = 0.030236298218369484\n",
      "Epoch #147 --> train_loss = 0.030236298218369484, val_loss = 0.0302097387611866\n",
      "Epoch #148 --> train_loss = 0.0302097387611866, val_loss = 0.030183307826519012\n",
      "Epoch #149 --> train_loss = 0.030183307826519012, val_loss = 0.030157014727592468\n",
      "Epoch #150 --> train_loss = 0.030157014727592468, val_loss = 0.030130848288536072\n",
      "Epoch #151 --> train_loss = 0.030130848288536072, val_loss = 0.03010481782257557\n",
      "Epoch #152 --> train_loss = 0.03010481782257557, val_loss = 0.030078914016485214\n",
      "Epoch #153 --> train_loss = 0.030078914016485214, val_loss = 0.030053146183490753\n",
      "Epoch #154 --> train_loss = 0.030053146183490753, val_loss = 0.03002750314772129\n",
      "Epoch #155 --> train_loss = 0.03002750314772129, val_loss = 0.030001990497112274\n",
      "Epoch #156 --> train_loss = 0.030001990497112274, val_loss = 0.029976608231663704\n",
      "Epoch #157 --> train_loss = 0.029976608231663704, val_loss = 0.029951347038149834\n",
      "Epoch #158 --> train_loss = 0.029951347038149834, val_loss = 0.029926221817731857\n",
      "Epoch #159 --> train_loss = 0.029926221817731857, val_loss = 0.02990121580660343\n",
      "Epoch #160 --> train_loss = 0.02990121580660343, val_loss = 0.029876340180635452\n",
      "Epoch #161 --> train_loss = 0.029876340180635452, val_loss = 0.02985158935189247\n",
      "Epoch #162 --> train_loss = 0.02985158935189247, val_loss = 0.02982696332037449\n",
      "Epoch #163 --> train_loss = 0.02982696332037449, val_loss = 0.029802458360791206\n",
      "Epoch #164 --> train_loss = 0.029802458360791206, val_loss = 0.029778078198432922\n",
      "Epoch #165 --> train_loss = 0.029778078198432922, val_loss = 0.029753820970654488\n",
      "Epoch #166 --> train_loss = 0.029753820970654488, val_loss = 0.029729684814810753\n",
      "Epoch #167 --> train_loss = 0.029729684814810753, val_loss = 0.029705673456192017\n",
      "Epoch #168 --> train_loss = 0.029705673456192017, val_loss = 0.02968178316950798\n",
      "Epoch #169 --> train_loss = 0.02968178316950798, val_loss = 0.029658008366823196\n",
      "Epoch #170 --> train_loss = 0.029658008366823196, val_loss = 0.029634352773427963\n",
      "Epoch #171 --> train_loss = 0.029634352773427963, val_loss = 0.02961082197725773\n",
      "Epoch #172 --> train_loss = 0.02961082197725773, val_loss = 0.029587402939796448\n",
      "Epoch #173 --> train_loss = 0.029587402939796448, val_loss = 0.029564106836915016\n",
      "Epoch #174 --> train_loss = 0.029564106836915016, val_loss = 0.029540926218032837\n",
      "Epoch #175 --> train_loss = 0.029540926218032837, val_loss = 0.02951786108314991\n",
      "Epoch #176 --> train_loss = 0.02951786108314991, val_loss = 0.029494915157556534\n",
      "Epoch #177 --> train_loss = 0.029494915157556534, val_loss = 0.02947208471596241\n",
      "Epoch #178 --> train_loss = 0.02947208471596241, val_loss = 0.02944936603307724\n",
      "Epoch #179 --> train_loss = 0.02944936603307724, val_loss = 0.02942676655948162\n",
      "Epoch #180 --> train_loss = 0.02942676655948162, val_loss = 0.029404284432530403\n",
      "Epoch #181 --> train_loss = 0.029404284432530403, val_loss = 0.02938191033899784\n",
      "Epoch #182 --> train_loss = 0.02938191033899784, val_loss = 0.029359644278883934\n",
      "Epoch #183 --> train_loss = 0.029359644278883934, val_loss = 0.02933749556541443\n",
      "Epoch #184 --> train_loss = 0.02933749556541443, val_loss = 0.029315458610653877\n",
      "Epoch #185 --> train_loss = 0.029315458610653877, val_loss = 0.02929353341460228\n",
      "Epoch #186 --> train_loss = 0.02929353341460228, val_loss = 0.029271714389324188\n",
      "Epoch #187 --> train_loss = 0.029271714389324188, val_loss = 0.02925000712275505\n",
      "Epoch #188 --> train_loss = 0.02925000712275505, val_loss = 0.029228409752249718\n",
      "Epoch #189 --> train_loss = 0.029228409752249718, val_loss = 0.029206926003098488\n",
      "Epoch #190 --> train_loss = 0.029206926003098488, val_loss = 0.029185548424720764\n",
      "Epoch #191 --> train_loss = 0.029185548424720764, val_loss = 0.029164273291826248\n",
      "Epoch #192 --> train_loss = 0.029164273291826248, val_loss = 0.029143109917640686\n",
      "Epoch #193 --> train_loss = 0.029143109917640686, val_loss = 0.02912205085158348\n",
      "Epoch #194 --> train_loss = 0.02912205085158348, val_loss = 0.029101096093654633\n",
      "Epoch #195 --> train_loss = 0.029101096093654633, val_loss = 0.029080253094434738\n",
      "Epoch #196 --> train_loss = 0.029080253094434738, val_loss = 0.029059510678052902\n",
      "Epoch #197 --> train_loss = 0.029059510678052902, val_loss = 0.029038870707154274\n",
      "Epoch #198 --> train_loss = 0.029038870707154274, val_loss = 0.0290183424949646\n",
      "Epoch #199 --> train_loss = 0.0290183424949646, val_loss = 0.028997913002967834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/armin/.local/lib/python3.8/site-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "net = Net(5, 1)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "optimizer.zero_grad()\n",
    "for epoch in range(200):\n",
    "    prediction = net(x_train)\n",
    "    loss = loss_func(prediction, y_train)\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()\n",
    "    val_predict = net (x_train)\n",
    "    val_loss = loss_func(val_predict, y_train)\n",
    "    print(f\"Epoch #{epoch} --> train_loss = {loss}, val_loss = {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2 (nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(Net2, self).__init__()\n",
    "        self.hidden1 = nn.Linear(n_inputs, 8)\n",
    "        self.hidden2 = nn.Linear(8, 4)\n",
    "        self.hidden3 = nn.Linear(4, 2)\n",
    "        self.predict = nn.Linear(2, n_outputs)\n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.hidden3(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.predict(x)\n",
    "        x = F.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/armin/.local/lib/python3.8/site-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 --> train_loss = 0.6256612539291382, val_loss = 0.623363733291626\n",
      "Epoch #1 --> train_loss = 0.623363733291626, val_loss = 0.6210651397705078\n",
      "Epoch #2 --> train_loss = 0.6210651397705078, val_loss = 0.6187654733657837\n",
      "Epoch #3 --> train_loss = 0.6187654733657837, val_loss = 0.6164647936820984\n",
      "Epoch #4 --> train_loss = 0.6164647936820984, val_loss = 0.6141631603240967\n",
      "Epoch #5 --> train_loss = 0.6141631603240967, val_loss = 0.6118608117103577\n",
      "Epoch #6 --> train_loss = 0.6118608117103577, val_loss = 0.609557569026947\n",
      "Epoch #7 --> train_loss = 0.609557569026947, val_loss = 0.6072534918785095\n",
      "Epoch #8 --> train_loss = 0.6072534918785095, val_loss = 0.6049487590789795\n",
      "Epoch #9 --> train_loss = 0.6049487590789795, val_loss = 0.6026434302330017\n",
      "Epoch #10 --> train_loss = 0.6026434302330017, val_loss = 0.6003375053405762\n",
      "Epoch #11 --> train_loss = 0.6003375053405762, val_loss = 0.5980311632156372\n",
      "Epoch #12 --> train_loss = 0.5980311632156372, val_loss = 0.5957242846488953\n",
      "Epoch #13 --> train_loss = 0.5957242846488953, val_loss = 0.5934171080589294\n",
      "Epoch #14 --> train_loss = 0.5934171080589294, val_loss = 0.591109573841095\n",
      "Epoch #15 --> train_loss = 0.591109573841095, val_loss = 0.5888018608093262\n",
      "Epoch #16 --> train_loss = 0.5888018608093262, val_loss = 0.586493968963623\n",
      "Epoch #17 --> train_loss = 0.586493968963623, val_loss = 0.5841858983039856\n",
      "Epoch #18 --> train_loss = 0.5841858983039856, val_loss = 0.5818778276443481\n",
      "Epoch #19 --> train_loss = 0.5818778276443481, val_loss = 0.5795696973800659\n",
      "Epoch #20 --> train_loss = 0.5795696973800659, val_loss = 0.5772616863250732\n",
      "Epoch #21 --> train_loss = 0.5772616863250732, val_loss = 0.5749538540840149\n",
      "Epoch #22 --> train_loss = 0.5749538540840149, val_loss = 0.5726460814476013\n",
      "Epoch #23 --> train_loss = 0.5726460814476013, val_loss = 0.5703387260437012\n",
      "Epoch #24 --> train_loss = 0.5703387260437012, val_loss = 0.5680317282676697\n",
      "Epoch #25 --> train_loss = 0.5680317282676697, val_loss = 0.5657251477241516\n",
      "Epoch #26 --> train_loss = 0.5657251477241516, val_loss = 0.5634190440177917\n",
      "Epoch #27 --> train_loss = 0.5634190440177917, val_loss = 0.5611135363578796\n",
      "Epoch #28 --> train_loss = 0.5611135363578796, val_loss = 0.5588085651397705\n",
      "Epoch #29 --> train_loss = 0.5588085651397705, val_loss = 0.5565042495727539\n",
      "Epoch #30 --> train_loss = 0.5565042495727539, val_loss = 0.5542007684707642\n",
      "Epoch #31 --> train_loss = 0.5542007684707642, val_loss = 0.5518980026245117\n",
      "Epoch #32 --> train_loss = 0.5518980026245117, val_loss = 0.5495962500572205\n",
      "Epoch #33 --> train_loss = 0.5495962500572205, val_loss = 0.547295331954956\n",
      "Epoch #34 --> train_loss = 0.547295331954956, val_loss = 0.5449955463409424\n",
      "Epoch #35 --> train_loss = 0.5449955463409424, val_loss = 0.5426968336105347\n",
      "Epoch #36 --> train_loss = 0.5426968336105347, val_loss = 0.5403991937637329\n",
      "Epoch #37 --> train_loss = 0.5403991937637329, val_loss = 0.5381028056144714\n",
      "Epoch #38 --> train_loss = 0.5381028056144714, val_loss = 0.5358077883720398\n",
      "Epoch #39 --> train_loss = 0.5358077883720398, val_loss = 0.5335140824317932\n",
      "Epoch #40 --> train_loss = 0.5335140824317932, val_loss = 0.5312218070030212\n",
      "Epoch #41 --> train_loss = 0.5312218070030212, val_loss = 0.5289311408996582\n",
      "Epoch #42 --> train_loss = 0.5289311408996582, val_loss = 0.5266419053077698\n",
      "Epoch #43 --> train_loss = 0.5266419053077698, val_loss = 0.5243543982505798\n",
      "Epoch #44 --> train_loss = 0.5243543982505798, val_loss = 0.5220686197280884\n",
      "Epoch #45 --> train_loss = 0.5220686197280884, val_loss = 0.5197845697402954\n",
      "Epoch #46 --> train_loss = 0.5197845697402954, val_loss = 0.5175023674964905\n",
      "Epoch #47 --> train_loss = 0.5175023674964905, val_loss = 0.5152221322059631\n",
      "Epoch #48 --> train_loss = 0.5152221322059631, val_loss = 0.5129438638687134\n",
      "Epoch #49 --> train_loss = 0.5129438638687134, val_loss = 0.5106676816940308\n",
      "Epoch #50 --> train_loss = 0.5106676816940308, val_loss = 0.5083935856819153\n",
      "Epoch #51 --> train_loss = 0.5083935856819153, val_loss = 0.5061216950416565\n",
      "Epoch #52 --> train_loss = 0.5061216950416565, val_loss = 0.5038520097732544\n",
      "Epoch #53 --> train_loss = 0.5038520097732544, val_loss = 0.5015847086906433\n",
      "Epoch #54 --> train_loss = 0.5015847086906433, val_loss = 0.499319851398468\n",
      "Epoch #55 --> train_loss = 0.499319851398468, val_loss = 0.49705737829208374\n",
      "Epoch #56 --> train_loss = 0.49705737829208374, val_loss = 0.4947975277900696\n",
      "Epoch #57 --> train_loss = 0.4947975277900696, val_loss = 0.49254027009010315\n",
      "Epoch #58 --> train_loss = 0.49254027009010315, val_loss = 0.49028563499450684\n",
      "Epoch #59 --> train_loss = 0.49028563499450684, val_loss = 0.4880337715148926\n",
      "Epoch #60 --> train_loss = 0.4880337715148926, val_loss = 0.48578476905822754\n",
      "Epoch #61 --> train_loss = 0.48578476905822754, val_loss = 0.4835386574268341\n",
      "Epoch #62 --> train_loss = 0.4835386574268341, val_loss = 0.48129552602767944\n",
      "Epoch #63 --> train_loss = 0.48129552602767944, val_loss = 0.4790554344654083\n",
      "Epoch #64 --> train_loss = 0.4790554344654083, val_loss = 0.47681835293769836\n",
      "Epoch #65 --> train_loss = 0.47681835293769836, val_loss = 0.4745844900608063\n",
      "Epoch #66 --> train_loss = 0.4745844900608063, val_loss = 0.47235384583473206\n",
      "Epoch #67 --> train_loss = 0.47235384583473206, val_loss = 0.4701264798641205\n",
      "Epoch #68 --> train_loss = 0.4701264798641205, val_loss = 0.46790239214897156\n",
      "Epoch #69 --> train_loss = 0.46790239214897156, val_loss = 0.46568185091018677\n",
      "Epoch #70 --> train_loss = 0.46568185091018677, val_loss = 0.4634646475315094\n",
      "Epoch #71 --> train_loss = 0.4634646475315094, val_loss = 0.46125122904777527\n",
      "Epoch #72 --> train_loss = 0.46125122904777527, val_loss = 0.4590412676334381\n",
      "Epoch #73 --> train_loss = 0.4590412676334381, val_loss = 0.4568350613117218\n",
      "Epoch #74 --> train_loss = 0.4568350613117218, val_loss = 0.45463261008262634\n",
      "Epoch #75 --> train_loss = 0.45463261008262634, val_loss = 0.4524339437484741\n",
      "Epoch #76 --> train_loss = 0.4524339437484741, val_loss = 0.4502392113208771\n",
      "Epoch #77 --> train_loss = 0.4502392113208771, val_loss = 0.4480484127998352\n",
      "Epoch #78 --> train_loss = 0.4480484127998352, val_loss = 0.4458616077899933\n",
      "Epoch #79 --> train_loss = 0.4458616077899933, val_loss = 0.44367897510528564\n",
      "Epoch #80 --> train_loss = 0.44367897510528564, val_loss = 0.44150033593177795\n",
      "Epoch #81 --> train_loss = 0.44150033593177795, val_loss = 0.4393260180950165\n",
      "Epoch #82 --> train_loss = 0.4393260180950165, val_loss = 0.43715590238571167\n",
      "Epoch #83 --> train_loss = 0.43715590238571167, val_loss = 0.43499019742012024\n",
      "Epoch #84 --> train_loss = 0.43499019742012024, val_loss = 0.4328288435935974\n",
      "Epoch #85 --> train_loss = 0.4328288435935974, val_loss = 0.4306720197200775\n",
      "Epoch #86 --> train_loss = 0.4306720197200775, val_loss = 0.4285195767879486\n",
      "Epoch #87 --> train_loss = 0.4285195767879486, val_loss = 0.42637184262275696\n",
      "Epoch #88 --> train_loss = 0.42637184262275696, val_loss = 0.4242286682128906\n",
      "Epoch #89 --> train_loss = 0.4242286682128906, val_loss = 0.42209017276763916\n",
      "Epoch #90 --> train_loss = 0.42209017276763916, val_loss = 0.4199564754962921\n",
      "Epoch #91 --> train_loss = 0.4199564754962921, val_loss = 0.41782766580581665\n",
      "Epoch #92 --> train_loss = 0.41782766580581665, val_loss = 0.4157036542892456\n",
      "Epoch #93 --> train_loss = 0.4157036542892456, val_loss = 0.4135846793651581\n",
      "Epoch #94 --> train_loss = 0.4135846793651581, val_loss = 0.4114706814289093\n",
      "Epoch #95 --> train_loss = 0.4114706814289093, val_loss = 0.40936172008514404\n",
      "Epoch #96 --> train_loss = 0.40936172008514404, val_loss = 0.4072577655315399\n",
      "Epoch #97 --> train_loss = 0.4072577655315399, val_loss = 0.4051591157913208\n",
      "Epoch #98 --> train_loss = 0.4051591157913208, val_loss = 0.40306568145751953\n",
      "Epoch #99 --> train_loss = 0.40306568145751953, val_loss = 0.4009774625301361\n",
      "Epoch #100 --> train_loss = 0.4009774625301361, val_loss = 0.3988945782184601\n",
      "Epoch #101 --> train_loss = 0.3988945782184601, val_loss = 0.39681705832481384\n",
      "Epoch #102 --> train_loss = 0.39681705832481384, val_loss = 0.3947450518608093\n",
      "Epoch #103 --> train_loss = 0.3947450518608093, val_loss = 0.39267849922180176\n",
      "Epoch #104 --> train_loss = 0.39267849922180176, val_loss = 0.3906175196170807\n",
      "Epoch #105 --> train_loss = 0.3906175196170807, val_loss = 0.3885621130466461\n",
      "Epoch #106 --> train_loss = 0.3885621130466461, val_loss = 0.3865123987197876\n",
      "Epoch #107 --> train_loss = 0.3865123987197876, val_loss = 0.3844684064388275\n",
      "Epoch #108 --> train_loss = 0.3844684064388275, val_loss = 0.3824301064014435\n",
      "Epoch #109 --> train_loss = 0.3824301064014435, val_loss = 0.38039758801460266\n",
      "Epoch #110 --> train_loss = 0.38039758801460266, val_loss = 0.3783709704875946\n",
      "Epoch #111 --> train_loss = 0.3783709704875946, val_loss = 0.3763502538204193\n",
      "Epoch #112 --> train_loss = 0.3763502538204193, val_loss = 0.37433549761772156\n",
      "Epoch #113 --> train_loss = 0.37433549761772156, val_loss = 0.37232670187950134\n",
      "Epoch #114 --> train_loss = 0.37232670187950134, val_loss = 0.37032395601272583\n",
      "Epoch #115 --> train_loss = 0.37032395601272583, val_loss = 0.3683272898197174\n",
      "Epoch #116 --> train_loss = 0.3683272898197174, val_loss = 0.3663368225097656\n",
      "Epoch #117 --> train_loss = 0.3663368225097656, val_loss = 0.3643524944782257\n",
      "Epoch #118 --> train_loss = 0.3643524944782257, val_loss = 0.3623744249343872\n",
      "Epoch #119 --> train_loss = 0.3623744249343872, val_loss = 0.3604026436805725\n",
      "Epoch #120 --> train_loss = 0.3604026436805725, val_loss = 0.358437180519104\n",
      "Epoch #121 --> train_loss = 0.358437180519104, val_loss = 0.3564780652523041\n",
      "Epoch #122 --> train_loss = 0.3564780652523041, val_loss = 0.35452529788017273\n",
      "Epoch #123 --> train_loss = 0.35452529788017273, val_loss = 0.3525789976119995\n",
      "Epoch #124 --> train_loss = 0.3525789976119995, val_loss = 0.3506391942501068\n",
      "Epoch #125 --> train_loss = 0.3506391942501068, val_loss = 0.348705917596817\n",
      "Epoch #126 --> train_loss = 0.348705917596817, val_loss = 0.3467791974544525\n",
      "Epoch #127 --> train_loss = 0.3467791974544525, val_loss = 0.3448590338230133\n",
      "Epoch #128 --> train_loss = 0.3448590338230133, val_loss = 0.34294554591178894\n",
      "Epoch #129 --> train_loss = 0.34294554591178894, val_loss = 0.3410387635231018\n",
      "Epoch #130 --> train_loss = 0.3410387635231018, val_loss = 0.3391386568546295\n",
      "Epoch #131 --> train_loss = 0.3391386568546295, val_loss = 0.33724528551101685\n",
      "Epoch #132 --> train_loss = 0.33724528551101685, val_loss = 0.33535870909690857\n",
      "Epoch #133 --> train_loss = 0.33535870909690857, val_loss = 0.3334788978099823\n",
      "Epoch #134 --> train_loss = 0.3334788978099823, val_loss = 0.33160603046417236\n",
      "Epoch #135 --> train_loss = 0.33160603046417236, val_loss = 0.3297399878501892\n",
      "Epoch #136 --> train_loss = 0.3297399878501892, val_loss = 0.327880859375\n",
      "Epoch #137 --> train_loss = 0.327880859375, val_loss = 0.3260286748409271\n",
      "Epoch #138 --> train_loss = 0.3260286748409271, val_loss = 0.32418346405029297\n",
      "Epoch #139 --> train_loss = 0.32418346405029297, val_loss = 0.3223452866077423\n",
      "Epoch #140 --> train_loss = 0.3223452866077423, val_loss = 0.32051411271095276\n",
      "Epoch #141 --> train_loss = 0.32051411271095276, val_loss = 0.31869006156921387\n",
      "Epoch #142 --> train_loss = 0.31869006156921387, val_loss = 0.31687304377555847\n",
      "Epoch #143 --> train_loss = 0.31687304377555847, val_loss = 0.3150632083415985\n",
      "Epoch #144 --> train_loss = 0.3150632083415985, val_loss = 0.3132604956626892\n",
      "Epoch #145 --> train_loss = 0.3132604956626892, val_loss = 0.31146496534347534\n",
      "Epoch #146 --> train_loss = 0.31146496534347534, val_loss = 0.30967655777931213\n",
      "Epoch #147 --> train_loss = 0.30967655777931213, val_loss = 0.3078954815864563\n",
      "Epoch #148 --> train_loss = 0.3078954815864563, val_loss = 0.3061215579509735\n",
      "Epoch #149 --> train_loss = 0.3061215579509735, val_loss = 0.3043549656867981\n",
      "Epoch #150 --> train_loss = 0.3043549656867981, val_loss = 0.3025956451892853\n",
      "Epoch #151 --> train_loss = 0.3025956451892853, val_loss = 0.3008436858654022\n",
      "Epoch #152 --> train_loss = 0.3008436858654022, val_loss = 0.29909899830818176\n",
      "Epoch #153 --> train_loss = 0.29909899830818176, val_loss = 0.29736173152923584\n",
      "Epoch #154 --> train_loss = 0.29736173152923584, val_loss = 0.2956318259239197\n",
      "Epoch #155 --> train_loss = 0.2956318259239197, val_loss = 0.29390934109687805\n",
      "Epoch #156 --> train_loss = 0.29390934109687805, val_loss = 0.2921942472457886\n",
      "Epoch #157 --> train_loss = 0.2921942472457886, val_loss = 0.290486603975296\n",
      "Epoch #158 --> train_loss = 0.290486603975296, val_loss = 0.2887864112854004\n",
      "Epoch #159 --> train_loss = 0.2887864112854004, val_loss = 0.2870936691761017\n",
      "Epoch #160 --> train_loss = 0.2870936691761017, val_loss = 0.2854083776473999\n",
      "Epoch #161 --> train_loss = 0.2854083776473999, val_loss = 0.2837306261062622\n",
      "Epoch #162 --> train_loss = 0.2837306261062622, val_loss = 0.2820604145526886\n",
      "Epoch #163 --> train_loss = 0.2820604145526886, val_loss = 0.2803976833820343\n",
      "Epoch #164 --> train_loss = 0.2803976833820343, val_loss = 0.2787424921989441\n",
      "Epoch #165 --> train_loss = 0.2787424921989441, val_loss = 0.27709487080574036\n",
      "Epoch #166 --> train_loss = 0.27709487080574036, val_loss = 0.2754547894001007\n",
      "Epoch #167 --> train_loss = 0.2754547894001007, val_loss = 0.27382227778434753\n",
      "Epoch #168 --> train_loss = 0.27382227778434753, val_loss = 0.2721973955631256\n",
      "Epoch #169 --> train_loss = 0.2721973955631256, val_loss = 0.2705800533294678\n",
      "Epoch #170 --> train_loss = 0.2705800533294678, val_loss = 0.2689703404903412\n",
      "Epoch #171 --> train_loss = 0.2689703404903412, val_loss = 0.26736822724342346\n",
      "Epoch #172 --> train_loss = 0.26736822724342346, val_loss = 0.265773743391037\n",
      "Epoch #173 --> train_loss = 0.265773743391037, val_loss = 0.264186829328537\n",
      "Epoch #174 --> train_loss = 0.264186829328537, val_loss = 0.262607604265213\n",
      "Epoch #175 --> train_loss = 0.262607604265213, val_loss = 0.2610359787940979\n",
      "Epoch #176 --> train_loss = 0.2610359787940979, val_loss = 0.2594720125198364\n",
      "Epoch #177 --> train_loss = 0.2594720125198364, val_loss = 0.2579156458377838\n",
      "Epoch #178 --> train_loss = 0.2579156458377838, val_loss = 0.2563669979572296\n",
      "Epoch #179 --> train_loss = 0.2563669979572296, val_loss = 0.2548259496688843\n",
      "Epoch #180 --> train_loss = 0.2548259496688843, val_loss = 0.25329259037971497\n",
      "Epoch #181 --> train_loss = 0.25329259037971497, val_loss = 0.2517668902873993\n",
      "Epoch #182 --> train_loss = 0.2517668902873993, val_loss = 0.25024881958961487\n",
      "Epoch #183 --> train_loss = 0.25024881958961487, val_loss = 0.2487383782863617\n",
      "Epoch #184 --> train_loss = 0.2487383782863617, val_loss = 0.24723565578460693\n",
      "Epoch #185 --> train_loss = 0.24723565578460693, val_loss = 0.24574056267738342\n",
      "Epoch #186 --> train_loss = 0.24574056267738342, val_loss = 0.24425312876701355\n",
      "Epoch #187 --> train_loss = 0.24425312876701355, val_loss = 0.24277332425117493\n",
      "Epoch #188 --> train_loss = 0.24277332425117493, val_loss = 0.24130119383335114\n",
      "Epoch #189 --> train_loss = 0.24130119383335114, val_loss = 0.2398367077112198\n",
      "Epoch #190 --> train_loss = 0.2398367077112198, val_loss = 0.2383798509836197\n",
      "Epoch #191 --> train_loss = 0.2383798509836197, val_loss = 0.23693063855171204\n",
      "Epoch #192 --> train_loss = 0.23693063855171204, val_loss = 0.23548907041549683\n",
      "Epoch #193 --> train_loss = 0.23548907041549683, val_loss = 0.23405513167381287\n",
      "Epoch #194 --> train_loss = 0.23405513167381287, val_loss = 0.23262883722782135\n",
      "Epoch #195 --> train_loss = 0.23262883722782135, val_loss = 0.2312101274728775\n",
      "Epoch #196 --> train_loss = 0.2312101274728775, val_loss = 0.2297990471124649\n",
      "Epoch #197 --> train_loss = 0.2297990471124649, val_loss = 0.22839558124542236\n",
      "Epoch #198 --> train_loss = 0.22839558124542236, val_loss = 0.2269996851682663\n",
      "Epoch #199 --> train_loss = 0.2269996851682663, val_loss = 0.22561140358448029\n"
     ]
    }
   ],
   "source": [
    "net2 = Net2(5, 1)\n",
    "optimizer = torch.optim.SGD(net2.parameters(), lr=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "optimizer.zero_grad()\n",
    "for epoch in range(200):\n",
    "    prediction = net2(x_train)\n",
    "    loss = loss_func(prediction, y_train)\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()\n",
    "    val_predict = net2 (x_train)\n",
    "    val_loss = loss_func(val_predict, y_train)\n",
    "    print(f\"Epoch #{epoch} --> train_loss = {loss}, val_loss = {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: variables __flops__ or __params__ are already defined for the moduleLinear ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleLinear ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleLinear ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleLinear ptflops can affect your code!\n",
      "Warning: module Net2 is treated as a zero-op.\n",
      "Net2(\n",
      "  0.0 M, 100.000% Params, 0.0 GMac, 100.000% MACs, \n",
      "  (hidden1): Linear(0.0 M, 49.485% Params, 0.0 GMac, 49.485% MACs, in_features=5, out_features=8, bias=True)\n",
      "  (hidden2): Linear(0.0 M, 37.113% Params, 0.0 GMac, 37.113% MACs, in_features=8, out_features=4, bias=True)\n",
      "  (hidden3): Linear(0.0 M, 10.309% Params, 0.0 GMac, 10.309% MACs, in_features=4, out_features=2, bias=True)\n",
      "  (predict): Linear(0.0 M, 3.093% Params, 0.0 GMac, 3.093% MACs, in_features=2, out_features=1, bias=True)\n",
      ")\n",
      "Computational complexity:       0.0 GMac\n",
      "Number of parameters:           97      \n"
     ]
    }
   ],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "macs, params = get_model_complexity_info(net2, (1,5), \n",
    "                                           as_strings=True,\n",
    "                                           print_per_layer_stat=True, \n",
    "                                           verbose=True)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module Net is treated as a zero-op.\n",
      "Net(\n",
      "  0.0 M, 100.000% Params, 0.0 GMac, 100.000% MACs, \n",
      "  (hidden): Linear(0.0 M, 84.211% Params, 0.0 GMac, 84.211% MACs, in_features=5, out_features=8, bias=True)\n",
      "  (predict): Linear(0.0 M, 15.789% Params, 0.0 GMac, 15.789% MACs, in_features=8, out_features=1, bias=True)\n",
      ")\n",
      "Computational complexity:       0.0 GMac\n",
      "Number of parameters:           57      \n"
     ]
    }
   ],
   "source": [
    "macs, params = get_model_complexity_info(net, (1,5), \n",
    "                                           as_strings=True,\n",
    "                                           print_per_layer_stat=True, \n",
    "                                           verbose=True)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset CIFAR10\n",
       "     Number of datapoints: 50000\n",
       "     Root location: ./data\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "            ),\n",
       " Dataset CIFAR10\n",
       "     Number of datapoints: 10000\n",
       "     Root location: ./data\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "            ))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "class_name = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "cifar10 = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
    "cifar10_val = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform)\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=True)\n",
    "cifar10, cifar10_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(3*32*32, 512)\n",
    "        self.predict = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.predict(x)\n",
    "        x = F.tanh(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 --> train_loss: 2.0045 | val_loss: 1.8200 | Validation Accuracy: 36.58%\n",
      "Epoch #1 --> train_loss: 1.8976 | val_loss: 2.2250 | Validation Accuracy: 37.55%\n",
      "Epoch #2 --> train_loss: 1.8206 | val_loss: 1.9687 | Validation Accuracy: 38.42%\n",
      "Epoch #3 --> train_loss: 1.9344 | val_loss: 1.5311 | Validation Accuracy: 38.68%\n",
      "Epoch #4 --> train_loss: 2.0381 | val_loss: 2.0096 | Validation Accuracy: 38.73%\n",
      "Epoch #5 --> train_loss: 1.6628 | val_loss: 2.0216 | Validation Accuracy: 39.26%\n",
      "Epoch #6 --> train_loss: 1.7598 | val_loss: 1.9807 | Validation Accuracy: 39.629999999999995%\n",
      "Epoch #7 --> train_loss: 1.7964 | val_loss: 1.7895 | Validation Accuracy: 40.26%\n",
      "Epoch #8 --> train_loss: 1.6592 | val_loss: 1.8183 | Validation Accuracy: 40.52%\n",
      "Epoch #9 --> train_loss: 1.6561 | val_loss: 1.9971 | Validation Accuracy: 40.03%\n",
      "Epoch #10 --> train_loss: 1.4846 | val_loss: 1.7445 | Validation Accuracy: 41.31%\n",
      "Epoch #11 --> train_loss: 2.0398 | val_loss: 1.7089 | Validation Accuracy: 41.4%\n",
      "Epoch #12 --> train_loss: 1.4475 | val_loss: 1.8470 | Validation Accuracy: 41.27%\n",
      "Epoch #13 --> train_loss: 1.4222 | val_loss: 1.7342 | Validation Accuracy: 42.02%\n",
      "Epoch #14 --> train_loss: 1.7204 | val_loss: 1.8485 | Validation Accuracy: 41.47%\n",
      "Epoch #15 --> train_loss: 1.6544 | val_loss: 1.5900 | Validation Accuracy: 42.17%\n",
      "Epoch #16 --> train_loss: 1.7930 | val_loss: 1.8143 | Validation Accuracy: 42.41%\n",
      "Epoch #17 --> train_loss: 1.7308 | val_loss: 1.9359 | Validation Accuracy: 42.76%\n",
      "Epoch #18 --> train_loss: 1.6116 | val_loss: 1.5594 | Validation Accuracy: 43.41%\n",
      "Epoch #19 --> train_loss: 1.7388 | val_loss: 1.7336 | Validation Accuracy: 42.83%\n",
      "Epoch #20 --> train_loss: 1.7627 | val_loss: 1.8444 | Validation Accuracy: 43.22%\n",
      "Epoch #21 --> train_loss: 1.8343 | val_loss: 1.7781 | Validation Accuracy: 43.68%\n",
      "Epoch #22 --> train_loss: 1.7192 | val_loss: 1.5753 | Validation Accuracy: 43.730000000000004%\n",
      "Epoch #23 --> train_loss: 1.5182 | val_loss: 1.6052 | Validation Accuracy: 44.26%\n",
      "Epoch #24 --> train_loss: 1.4373 | val_loss: 1.6996 | Validation Accuracy: 44.529999999999994%\n",
      "Epoch #25 --> train_loss: 1.6759 | val_loss: 1.8451 | Validation Accuracy: 44.519999999999996%\n",
      "Epoch #26 --> train_loss: 1.7559 | val_loss: 1.7313 | Validation Accuracy: 45.16%\n",
      "Epoch #27 --> train_loss: 1.8758 | val_loss: 1.8196 | Validation Accuracy: 45.17%\n",
      "Epoch #28 --> train_loss: 1.7259 | val_loss: 1.5277 | Validation Accuracy: 45.28%\n",
      "Epoch #29 --> train_loss: 1.6806 | val_loss: 1.5751 | Validation Accuracy: 45.72%\n",
      "Epoch #30 --> train_loss: 1.5575 | val_loss: 1.4866 | Validation Accuracy: 45.72%\n",
      "Epoch #31 --> train_loss: 1.6375 | val_loss: 1.8402 | Validation Accuracy: 46.150000000000006%\n",
      "Epoch #32 --> train_loss: 1.5758 | val_loss: 1.9027 | Validation Accuracy: 46.08%\n",
      "Epoch #33 --> train_loss: 1.7309 | val_loss: 1.6614 | Validation Accuracy: 46.43%\n",
      "Epoch #34 --> train_loss: 1.5742 | val_loss: 1.8468 | Validation Accuracy: 47.11%\n",
      "Epoch #35 --> train_loss: 1.6906 | val_loss: 1.6367 | Validation Accuracy: 46.89%\n",
      "Epoch #36 --> train_loss: 1.4824 | val_loss: 1.5797 | Validation Accuracy: 47.4%\n",
      "Epoch #37 --> train_loss: 1.3176 | val_loss: 1.5766 | Validation Accuracy: 47.57%\n",
      "Epoch #38 --> train_loss: 1.6056 | val_loss: 1.3779 | Validation Accuracy: 47.31%\n",
      "Epoch #39 --> train_loss: 1.7550 | val_loss: 1.8769 | Validation Accuracy: 48.230000000000004%\n",
      "Epoch #40 --> train_loss: 1.3837 | val_loss: 1.5953 | Validation Accuracy: 47.96%\n",
      "Epoch #41 --> train_loss: 1.5775 | val_loss: 1.5139 | Validation Accuracy: 48.0%\n",
      "Epoch #42 --> train_loss: 1.5699 | val_loss: 1.4894 | Validation Accuracy: 47.72%\n",
      "Epoch #43 --> train_loss: 1.3241 | val_loss: 1.7017 | Validation Accuracy: 47.93%\n",
      "Epoch #44 --> train_loss: 1.3599 | val_loss: 1.7189 | Validation Accuracy: 48.19%\n",
      "Epoch #45 --> train_loss: 1.4602 | val_loss: 1.6987 | Validation Accuracy: 48.27%\n",
      "Epoch #46 --> train_loss: 1.6195 | val_loss: 1.6507 | Validation Accuracy: 48.17%\n",
      "Epoch #47 --> train_loss: 1.3977 | val_loss: 1.6790 | Validation Accuracy: 48.089999999999996%\n",
      "Epoch #48 --> train_loss: 1.6627 | val_loss: 1.7807 | Validation Accuracy: 48.82%\n",
      "Epoch #49 --> train_loss: 1.4136 | val_loss: 1.5403 | Validation Accuracy: 48.61%\n",
      "Epoch #50 --> train_loss: 1.5202 | val_loss: 1.9702 | Validation Accuracy: 48.83%\n",
      "Epoch #51 --> train_loss: 1.8076 | val_loss: 1.2080 | Validation Accuracy: 48.66%\n",
      "Epoch #52 --> train_loss: 1.4742 | val_loss: 1.9190 | Validation Accuracy: 48.9%\n",
      "Epoch #53 --> train_loss: 1.6988 | val_loss: 1.7580 | Validation Accuracy: 48.88%\n",
      "Epoch #54 --> train_loss: 1.4726 | val_loss: 1.4597 | Validation Accuracy: 48.949999999999996%\n",
      "Epoch #55 --> train_loss: 1.5801 | val_loss: 1.5603 | Validation Accuracy: 48.949999999999996%\n",
      "Epoch #56 --> train_loss: 1.3796 | val_loss: 1.7264 | Validation Accuracy: 49.41%\n",
      "Epoch #57 --> train_loss: 1.3249 | val_loss: 1.6446 | Validation Accuracy: 49.33%\n",
      "Epoch #58 --> train_loss: 1.3937 | val_loss: 1.9422 | Validation Accuracy: 49.39%\n",
      "Epoch #59 --> train_loss: 1.5366 | val_loss: 1.9017 | Validation Accuracy: 49.3%\n",
      "Epoch #60 --> train_loss: 1.4719 | val_loss: 1.3539 | Validation Accuracy: 49.26%\n",
      "Epoch #61 --> train_loss: 1.3713 | val_loss: 1.7480 | Validation Accuracy: 49.79%\n",
      "Epoch #62 --> train_loss: 1.4978 | val_loss: 2.0426 | Validation Accuracy: 49.78%\n",
      "Epoch #63 --> train_loss: 1.3468 | val_loss: 1.6401 | Validation Accuracy: 49.19%\n",
      "Epoch #64 --> train_loss: 1.4116 | val_loss: 1.5578 | Validation Accuracy: 49.8%\n",
      "Epoch #65 --> train_loss: 1.1968 | val_loss: 1.9012 | Validation Accuracy: 49.89%\n",
      "Epoch #66 --> train_loss: 1.4771 | val_loss: 1.9659 | Validation Accuracy: 49.97%\n",
      "Epoch #67 --> train_loss: 1.5135 | val_loss: 1.5483 | Validation Accuracy: 49.53%\n",
      "Epoch #68 --> train_loss: 1.4006 | val_loss: 1.4217 | Validation Accuracy: 49.75%\n",
      "Epoch #69 --> train_loss: 1.4413 | val_loss: 1.6262 | Validation Accuracy: 49.980000000000004%\n",
      "Epoch #70 --> train_loss: 1.1949 | val_loss: 1.6801 | Validation Accuracy: 50.03999999999999%\n",
      "Epoch #71 --> train_loss: 1.5574 | val_loss: 1.8284 | Validation Accuracy: 49.82%\n",
      "Epoch #72 --> train_loss: 1.3679 | val_loss: 1.6835 | Validation Accuracy: 50.17%\n",
      "Epoch #73 --> train_loss: 1.3751 | val_loss: 1.7700 | Validation Accuracy: 50.21%\n",
      "Epoch #74 --> train_loss: 1.4796 | val_loss: 1.7990 | Validation Accuracy: 50.260000000000005%\n",
      "Epoch #75 --> train_loss: 1.6704 | val_loss: 1.6697 | Validation Accuracy: 50.13999999999999%\n",
      "Epoch #76 --> train_loss: 1.4659 | val_loss: 1.5436 | Validation Accuracy: 50.1%\n",
      "Epoch #77 --> train_loss: 1.3328 | val_loss: 1.5642 | Validation Accuracy: 49.88%\n",
      "Epoch #78 --> train_loss: 1.1732 | val_loss: 2.0396 | Validation Accuracy: 49.75%\n",
      "Epoch #79 --> train_loss: 1.2675 | val_loss: 1.5034 | Validation Accuracy: 50.11%\n",
      "Epoch #80 --> train_loss: 1.2024 | val_loss: 1.5688 | Validation Accuracy: 50.18%\n",
      "Epoch #81 --> train_loss: 1.2163 | val_loss: 1.4479 | Validation Accuracy: 50.6%\n",
      "Epoch #82 --> train_loss: 1.2871 | val_loss: 1.5593 | Validation Accuracy: 50.019999999999996%\n",
      "Epoch #83 --> train_loss: 1.0539 | val_loss: 1.5276 | Validation Accuracy: 50.14999999999999%\n",
      "Epoch #84 --> train_loss: 1.3750 | val_loss: 1.8930 | Validation Accuracy: 50.51%\n",
      "Epoch #85 --> train_loss: 1.3080 | val_loss: 1.4683 | Validation Accuracy: 50.449999999999996%\n",
      "Epoch #86 --> train_loss: 1.2692 | val_loss: 1.6329 | Validation Accuracy: 50.31%\n",
      "Epoch #87 --> train_loss: 1.4060 | val_loss: 1.6728 | Validation Accuracy: 50.31%\n",
      "Epoch #88 --> train_loss: 1.2095 | val_loss: 1.4083 | Validation Accuracy: 50.260000000000005%\n",
      "Epoch #89 --> train_loss: 1.1318 | val_loss: 1.6935 | Validation Accuracy: 50.32%\n",
      "Epoch #90 --> train_loss: 1.3813 | val_loss: 1.7105 | Validation Accuracy: 50.580000000000005%\n",
      "Epoch #91 --> train_loss: 1.0263 | val_loss: 1.8111 | Validation Accuracy: 50.43%\n",
      "Epoch #92 --> train_loss: 1.4275 | val_loss: 1.2721 | Validation Accuracy: 50.51%\n",
      "Epoch #93 --> train_loss: 1.3936 | val_loss: 1.4676 | Validation Accuracy: 50.21%\n",
      "Epoch #94 --> train_loss: 1.1671 | val_loss: 1.4818 | Validation Accuracy: 50.470000000000006%\n",
      "Epoch #95 --> train_loss: 1.2181 | val_loss: 1.5259 | Validation Accuracy: 50.56%\n",
      "Epoch #96 --> train_loss: 1.3655 | val_loss: 1.7175 | Validation Accuracy: 50.49%\n",
      "Epoch #97 --> train_loss: 1.3379 | val_loss: 1.3369 | Validation Accuracy: 50.24999999999999%\n",
      "Epoch #98 --> train_loss: 1.3175 | val_loss: 1.4516 | Validation Accuracy: 50.43%\n",
      "Epoch #99 --> train_loss: 1.1998 | val_loss: 1.4762 | Validation Accuracy: 50.4%\n",
      "Epoch #100 --> train_loss: 1.0824 | val_loss: 1.4653 | Validation Accuracy: 50.42%\n",
      "Epoch #101 --> train_loss: 1.1808 | val_loss: 1.7675 | Validation Accuracy: 50.41%\n",
      "Epoch #102 --> train_loss: 1.1914 | val_loss: 2.1499 | Validation Accuracy: 50.4%\n",
      "Epoch #103 --> train_loss: 1.3207 | val_loss: 1.7236 | Validation Accuracy: 50.86000000000001%\n",
      "Epoch #104 --> train_loss: 1.3156 | val_loss: 1.6286 | Validation Accuracy: 50.480000000000004%\n",
      "Epoch #105 --> train_loss: 1.1487 | val_loss: 1.8352 | Validation Accuracy: 50.43%\n",
      "Epoch #106 --> train_loss: 1.2339 | val_loss: 1.5283 | Validation Accuracy: 50.39%\n",
      "Epoch #107 --> train_loss: 1.2198 | val_loss: 1.6233 | Validation Accuracy: 50.05%\n",
      "Epoch #108 --> train_loss: 1.1058 | val_loss: 2.0023 | Validation Accuracy: 50.71%\n",
      "Epoch #109 --> train_loss: 1.4432 | val_loss: 1.4952 | Validation Accuracy: 50.44%\n",
      "Epoch #110 --> train_loss: 1.2261 | val_loss: 1.4945 | Validation Accuracy: 50.33%\n",
      "Epoch #111 --> train_loss: 1.0582 | val_loss: 1.5944 | Validation Accuracy: 50.4%\n",
      "Epoch #112 --> train_loss: 0.9713 | val_loss: 1.8787 | Validation Accuracy: 50.61%\n",
      "Epoch #113 --> train_loss: 1.0401 | val_loss: 1.7084 | Validation Accuracy: 50.349999999999994%\n",
      "Epoch #114 --> train_loss: 1.1163 | val_loss: 1.6732 | Validation Accuracy: 50.54%\n",
      "Epoch #115 --> train_loss: 1.1357 | val_loss: 1.6673 | Validation Accuracy: 50.39%\n",
      "Epoch #116 --> train_loss: 1.1461 | val_loss: 1.7361 | Validation Accuracy: 50.68%\n",
      "Epoch #117 --> train_loss: 1.1083 | val_loss: 1.6118 | Validation Accuracy: 50.74999999999999%\n",
      "Epoch #118 --> train_loss: 1.0757 | val_loss: 1.6037 | Validation Accuracy: 50.33%\n",
      "Epoch #119 --> train_loss: 1.3640 | val_loss: 1.8713 | Validation Accuracy: 50.55%\n",
      "Epoch #120 --> train_loss: 1.1996 | val_loss: 1.8473 | Validation Accuracy: 50.739999999999995%\n",
      "Epoch #121 --> train_loss: 1.0482 | val_loss: 1.5575 | Validation Accuracy: 50.739999999999995%\n",
      "Epoch #122 --> train_loss: 1.0158 | val_loss: 1.5340 | Validation Accuracy: 50.42%\n",
      "Epoch #123 --> train_loss: 1.0834 | val_loss: 1.6624 | Validation Accuracy: 50.739999999999995%\n",
      "Epoch #124 --> train_loss: 1.0233 | val_loss: 1.6562 | Validation Accuracy: 50.31%\n",
      "Epoch #125 --> train_loss: 1.3572 | val_loss: 1.7126 | Validation Accuracy: 50.839999999999996%\n",
      "Epoch #126 --> train_loss: 1.2632 | val_loss: 1.5560 | Validation Accuracy: 50.470000000000006%\n",
      "Epoch #127 --> train_loss: 1.1919 | val_loss: 1.7439 | Validation Accuracy: 50.73%\n",
      "Epoch #128 --> train_loss: 1.0179 | val_loss: 1.5092 | Validation Accuracy: 51.0%\n",
      "Epoch #129 --> train_loss: 1.2104 | val_loss: 1.7586 | Validation Accuracy: 51.27%\n",
      "Epoch #130 --> train_loss: 1.3358 | val_loss: 1.3005 | Validation Accuracy: 50.81%\n",
      "Epoch #131 --> train_loss: 1.1437 | val_loss: 1.4935 | Validation Accuracy: 51.0%\n",
      "Epoch #132 --> train_loss: 1.0915 | val_loss: 1.3107 | Validation Accuracy: 50.78%\n",
      "Epoch #133 --> train_loss: 1.0719 | val_loss: 1.4722 | Validation Accuracy: 50.51%\n",
      "Epoch #134 --> train_loss: 1.0383 | val_loss: 1.5595 | Validation Accuracy: 50.63999999999999%\n",
      "Epoch #135 --> train_loss: 1.3191 | val_loss: 1.8885 | Validation Accuracy: 50.6%\n",
      "Epoch #136 --> train_loss: 1.0020 | val_loss: 1.7196 | Validation Accuracy: 50.73%\n",
      "Epoch #137 --> train_loss: 1.3185 | val_loss: 1.5404 | Validation Accuracy: 50.54%\n",
      "Epoch #138 --> train_loss: 1.1915 | val_loss: 1.8797 | Validation Accuracy: 50.49%\n",
      "Epoch #139 --> train_loss: 1.2035 | val_loss: 1.7533 | Validation Accuracy: 50.160000000000004%\n",
      "Epoch #140 --> train_loss: 1.2613 | val_loss: 1.4268 | Validation Accuracy: 50.03999999999999%\n",
      "Epoch #141 --> train_loss: 1.1477 | val_loss: 1.7021 | Validation Accuracy: 50.59%\n",
      "Epoch #142 --> train_loss: 1.1606 | val_loss: 1.4687 | Validation Accuracy: 50.62%\n",
      "Epoch #143 --> train_loss: 1.1645 | val_loss: 1.5379 | Validation Accuracy: 50.77%\n",
      "Epoch #144 --> train_loss: 1.3095 | val_loss: 1.8064 | Validation Accuracy: 50.61%\n",
      "Epoch #145 --> train_loss: 1.0628 | val_loss: 1.8643 | Validation Accuracy: 50.41%\n",
      "Epoch #146 --> train_loss: 1.2311 | val_loss: 1.7386 | Validation Accuracy: 50.739999999999995%\n",
      "Epoch #147 --> train_loss: 1.0655 | val_loss: 1.9502 | Validation Accuracy: 50.82%\n",
      "Epoch #148 --> train_loss: 1.1087 | val_loss: 1.4444 | Validation Accuracy: 50.629999999999995%\n",
      "Epoch #149 --> train_loss: 1.1572 | val_loss: 1.5293 | Validation Accuracy: 50.7%\n",
      "Epoch #150 --> train_loss: 1.1736 | val_loss: 1.5482 | Validation Accuracy: 50.39%\n",
      "Epoch #151 --> train_loss: 0.9418 | val_loss: 1.5066 | Validation Accuracy: 50.73%\n",
      "Epoch #152 --> train_loss: 1.4187 | val_loss: 1.6365 | Validation Accuracy: 50.0%\n",
      "Epoch #153 --> train_loss: 1.2496 | val_loss: 1.9462 | Validation Accuracy: 50.739999999999995%\n",
      "Epoch #154 --> train_loss: 0.9891 | val_loss: 1.6462 | Validation Accuracy: 50.760000000000005%\n",
      "Epoch #155 --> train_loss: 0.9632 | val_loss: 2.0222 | Validation Accuracy: 50.88%\n",
      "Epoch #156 --> train_loss: 0.9489 | val_loss: 1.7788 | Validation Accuracy: 50.74999999999999%\n",
      "Epoch #157 --> train_loss: 1.1781 | val_loss: 1.7155 | Validation Accuracy: 50.74999999999999%\n",
      "Epoch #158 --> train_loss: 0.9748 | val_loss: 1.4036 | Validation Accuracy: 50.7%\n",
      "Epoch #159 --> train_loss: 0.9214 | val_loss: 1.4028 | Validation Accuracy: 50.629999999999995%\n",
      "Epoch #160 --> train_loss: 1.1962 | val_loss: 1.6333 | Validation Accuracy: 50.7%\n",
      "Epoch #161 --> train_loss: 1.2641 | val_loss: 1.2908 | Validation Accuracy: 50.739999999999995%\n",
      "Epoch #162 --> train_loss: 1.2793 | val_loss: 1.8771 | Validation Accuracy: 50.63999999999999%\n",
      "Epoch #163 --> train_loss: 1.0594 | val_loss: 1.3889 | Validation Accuracy: 50.33%\n",
      "Epoch #164 --> train_loss: 1.1601 | val_loss: 1.6063 | Validation Accuracy: 50.7%\n",
      "Epoch #165 --> train_loss: 1.1671 | val_loss: 2.1157 | Validation Accuracy: 51.03%\n",
      "Epoch #166 --> train_loss: 1.0715 | val_loss: 1.5879 | Validation Accuracy: 50.61%\n",
      "Epoch #167 --> train_loss: 0.9360 | val_loss: 1.8239 | Validation Accuracy: 50.67%\n",
      "Epoch #168 --> train_loss: 0.9734 | val_loss: 1.5372 | Validation Accuracy: 50.8%\n",
      "Epoch #169 --> train_loss: 1.4148 | val_loss: 1.2257 | Validation Accuracy: 50.64999999999999%\n",
      "Epoch #170 --> train_loss: 1.1642 | val_loss: 2.1757 | Validation Accuracy: 50.4%\n",
      "Epoch #171 --> train_loss: 1.1948 | val_loss: 1.9517 | Validation Accuracy: 50.18%\n",
      "Epoch #172 --> train_loss: 1.1836 | val_loss: 1.5054 | Validation Accuracy: 49.96%\n",
      "Epoch #173 --> train_loss: 1.0031 | val_loss: 1.7403 | Validation Accuracy: 50.83%\n",
      "Epoch #174 --> train_loss: 1.0205 | val_loss: 1.6725 | Validation Accuracy: 50.81%\n",
      "Epoch #175 --> train_loss: 1.1657 | val_loss: 1.2011 | Validation Accuracy: 50.46000000000001%\n",
      "Epoch #176 --> train_loss: 1.0199 | val_loss: 1.6752 | Validation Accuracy: 50.349999999999994%\n",
      "Epoch #177 --> train_loss: 1.3687 | val_loss: 1.7137 | Validation Accuracy: 50.4%\n",
      "Epoch #178 --> train_loss: 1.0315 | val_loss: 1.5543 | Validation Accuracy: 50.519999999999996%\n",
      "Epoch #179 --> train_loss: 0.9676 | val_loss: 1.2475 | Validation Accuracy: 50.370000000000005%\n",
      "Epoch #180 --> train_loss: 0.9403 | val_loss: 1.9415 | Validation Accuracy: 50.660000000000004%\n",
      "Epoch #181 --> train_loss: 0.9341 | val_loss: 1.3964 | Validation Accuracy: 50.46000000000001%\n",
      "Epoch #182 --> train_loss: 1.2429 | val_loss: 1.8869 | Validation Accuracy: 50.14999999999999%\n",
      "Epoch #183 --> train_loss: 1.0771 | val_loss: 1.7488 | Validation Accuracy: 50.55%\n",
      "Epoch #184 --> train_loss: 0.9193 | val_loss: 1.5552 | Validation Accuracy: 50.56%\n",
      "Epoch #185 --> train_loss: 0.9796 | val_loss: 1.7137 | Validation Accuracy: 50.349999999999994%\n",
      "Epoch #186 --> train_loss: 0.9489 | val_loss: 1.6102 | Validation Accuracy: 50.760000000000005%\n",
      "Epoch #187 --> train_loss: 1.1274 | val_loss: 1.4257 | Validation Accuracy: 50.77%\n",
      "Epoch #188 --> train_loss: 1.3055 | val_loss: 1.7206 | Validation Accuracy: 49.88%\n",
      "Epoch #189 --> train_loss: 0.9057 | val_loss: 1.8504 | Validation Accuracy: 50.67%\n",
      "Epoch #190 --> train_loss: 1.1617 | val_loss: 1.2696 | Validation Accuracy: 50.519999999999996%\n",
      "Epoch #191 --> train_loss: 0.9550 | val_loss: 1.6527 | Validation Accuracy: 50.43%\n",
      "Epoch #192 --> train_loss: 1.0737 | val_loss: 1.9216 | Validation Accuracy: 50.61%\n",
      "Epoch #193 --> train_loss: 0.9750 | val_loss: 1.6624 | Validation Accuracy: 50.88%\n",
      "Epoch #194 --> train_loss: 1.3810 | val_loss: 1.9539 | Validation Accuracy: 50.62%\n",
      "Epoch #195 --> train_loss: 0.9045 | val_loss: 1.4395 | Validation Accuracy: 50.580000000000005%\n",
      "Epoch #196 --> train_loss: 0.9658 | val_loss: 1.2828 | Validation Accuracy: 50.849999999999994%\n",
      "Epoch #197 --> train_loss: 1.2945 | val_loss: 1.8169 | Validation Accuracy: 50.49%\n",
      "Epoch #198 --> train_loss: 1.0850 | val_loss: 1.6836 | Validation Accuracy: 50.55%\n",
      "Epoch #199 --> train_loss: 0.9910 | val_loss: 1.7483 | Validation Accuracy: 50.519999999999996%\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "optimizer.zero_grad()\n",
    "for epoch in range(200):\n",
    "    loss = 0\n",
    "    val_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    Acc = 0\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = net(imgs.view(batch_size, -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            batch_size = imgs.shape[0]\n",
    "            outputs = outputs = net(imgs.view(batch_size, -1))\n",
    "            val_loss = loss_fn(outputs, labels)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "            Acc = (correct / total)*100\n",
    "    print(f\"Epoch #{epoch} --> train_loss: {loss.item():.4f} | val_loss: {val_loss.item():.4f} | Validation Accuracy: {Acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.hidden1 = nn.Linear(3*32*32, 512)\n",
    "        self.hidden2 = nn.Linear(512, 128)\n",
    "        self.hidden3 = nn.Linear(128, 32)\n",
    "        self.predict = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.hidden3(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.predict(x)\n",
    "        x = F.tanh(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/armin/.local/lib/python3.8/site-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 --> train_loss: 2.1353 | val_loss: 2.1035 | Validation Accuracy: 31.41%\n",
      "Epoch #1 --> train_loss: 1.8086 | val_loss: 1.8264 | Validation Accuracy: 33.1%\n",
      "Epoch #2 --> train_loss: 1.8830 | val_loss: 1.8245 | Validation Accuracy: 34.74%\n",
      "Epoch #3 --> train_loss: 1.8548 | val_loss: 1.8101 | Validation Accuracy: 36.370000000000005%\n",
      "Epoch #4 --> train_loss: 2.1736 | val_loss: 1.8715 | Validation Accuracy: 37.46%\n",
      "Epoch #5 --> train_loss: 2.0371 | val_loss: 1.8636 | Validation Accuracy: 38.45%\n",
      "Epoch #6 --> train_loss: 1.8180 | val_loss: 1.8129 | Validation Accuracy: 38.769999999999996%\n",
      "Epoch #7 --> train_loss: 1.8672 | val_loss: 1.9475 | Validation Accuracy: 39.28%\n",
      "Epoch #8 --> train_loss: 1.6835 | val_loss: 1.6991 | Validation Accuracy: 39.81%\n",
      "Epoch #9 --> train_loss: 1.9610 | val_loss: 1.5696 | Validation Accuracy: 40.6%\n",
      "Epoch #10 --> train_loss: 1.8710 | val_loss: 1.6140 | Validation Accuracy: 41.05%\n",
      "Epoch #11 --> train_loss: 1.8019 | val_loss: 2.0715 | Validation Accuracy: 41.31%\n",
      "Epoch #12 --> train_loss: 2.0040 | val_loss: 1.8971 | Validation Accuracy: 41.54%\n",
      "Epoch #13 --> train_loss: 1.6934 | val_loss: 1.7056 | Validation Accuracy: 41.839999999999996%\n",
      "Epoch #14 --> train_loss: 1.8292 | val_loss: 1.8001 | Validation Accuracy: 42.55%\n",
      "Epoch #15 --> train_loss: 1.8864 | val_loss: 2.0951 | Validation Accuracy: 42.94%\n",
      "Epoch #16 --> train_loss: 1.8560 | val_loss: 1.7133 | Validation Accuracy: 43.269999999999996%\n",
      "Epoch #17 --> train_loss: 1.7378 | val_loss: 1.9685 | Validation Accuracy: 43.76%\n",
      "Epoch #18 --> train_loss: 2.0300 | val_loss: 1.7391 | Validation Accuracy: 43.95%\n",
      "Epoch #19 --> train_loss: 1.9463 | val_loss: 1.7874 | Validation Accuracy: 44.01%\n",
      "Epoch #20 --> train_loss: 1.6083 | val_loss: 1.7884 | Validation Accuracy: 43.86%\n",
      "Epoch #21 --> train_loss: 1.7808 | val_loss: 1.6023 | Validation Accuracy: 44.56%\n",
      "Epoch #22 --> train_loss: 1.7102 | val_loss: 1.4156 | Validation Accuracy: 45.59%\n",
      "Epoch #23 --> train_loss: 1.5508 | val_loss: 1.9391 | Validation Accuracy: 45.550000000000004%\n",
      "Epoch #24 --> train_loss: 1.5605 | val_loss: 1.7881 | Validation Accuracy: 45.14%\n",
      "Epoch #25 --> train_loss: 1.5986 | val_loss: 1.6132 | Validation Accuracy: 46.52%\n",
      "Epoch #26 --> train_loss: 1.4438 | val_loss: 1.7248 | Validation Accuracy: 45.800000000000004%\n",
      "Epoch #27 --> train_loss: 1.6224 | val_loss: 1.4930 | Validation Accuracy: 44.940000000000005%\n",
      "Epoch #28 --> train_loss: 1.4993 | val_loss: 1.8357 | Validation Accuracy: 46.39%\n",
      "Epoch #29 --> train_loss: 1.5158 | val_loss: 1.7313 | Validation Accuracy: 45.2%\n",
      "Epoch #30 --> train_loss: 1.6715 | val_loss: 1.6443 | Validation Accuracy: 47.19%\n",
      "Epoch #31 --> train_loss: 1.6872 | val_loss: 1.6854 | Validation Accuracy: 47.19%\n",
      "Epoch #32 --> train_loss: 1.5017 | val_loss: 1.7765 | Validation Accuracy: 47.0%\n",
      "Epoch #33 --> train_loss: 1.5057 | val_loss: 1.8658 | Validation Accuracy: 46.18%\n",
      "Epoch #34 --> train_loss: 1.3459 | val_loss: 1.7132 | Validation Accuracy: 47.449999999999996%\n",
      "Epoch #35 --> train_loss: 1.5552 | val_loss: 1.8817 | Validation Accuracy: 46.67%\n",
      "Epoch #36 --> train_loss: 1.3083 | val_loss: 1.5019 | Validation Accuracy: 47.44%\n",
      "Epoch #37 --> train_loss: 1.7360 | val_loss: 1.5924 | Validation Accuracy: 48.24%\n",
      "Epoch #38 --> train_loss: 1.7606 | val_loss: 1.6588 | Validation Accuracy: 47.3%\n",
      "Epoch #39 --> train_loss: 1.7057 | val_loss: 1.9605 | Validation Accuracy: 48.02%\n",
      "Epoch #40 --> train_loss: 1.1926 | val_loss: 1.7780 | Validation Accuracy: 49.35%\n",
      "Epoch #41 --> train_loss: 1.2806 | val_loss: 1.7154 | Validation Accuracy: 47.85%\n",
      "Epoch #42 --> train_loss: 1.6463 | val_loss: 1.6460 | Validation Accuracy: 48.61%\n",
      "Epoch #43 --> train_loss: 1.5570 | val_loss: 1.5264 | Validation Accuracy: 48.61%\n",
      "Epoch #44 --> train_loss: 1.5016 | val_loss: 1.7541 | Validation Accuracy: 49.0%\n",
      "Epoch #45 --> train_loss: 1.6127 | val_loss: 1.6973 | Validation Accuracy: 48.209999999999994%\n",
      "Epoch #46 --> train_loss: 1.4056 | val_loss: 1.8489 | Validation Accuracy: 48.93%\n",
      "Epoch #47 --> train_loss: 1.5332 | val_loss: 1.4481 | Validation Accuracy: 49.980000000000004%\n",
      "Epoch #48 --> train_loss: 1.5399 | val_loss: 1.9006 | Validation Accuracy: 48.64%\n",
      "Epoch #49 --> train_loss: 1.4408 | val_loss: 1.6804 | Validation Accuracy: 49.08%\n",
      "Epoch #50 --> train_loss: 1.5784 | val_loss: 1.4962 | Validation Accuracy: 48.0%\n",
      "Epoch #51 --> train_loss: 1.8924 | val_loss: 1.6946 | Validation Accuracy: 49.49%\n",
      "Epoch #52 --> train_loss: 1.7653 | val_loss: 1.6946 | Validation Accuracy: 49.0%\n",
      "Epoch #53 --> train_loss: 1.1069 | val_loss: 1.5830 | Validation Accuracy: 49.68%\n",
      "Epoch #54 --> train_loss: 1.2939 | val_loss: 1.7785 | Validation Accuracy: 49.559999999999995%\n",
      "Epoch #55 --> train_loss: 1.4997 | val_loss: 1.7313 | Validation Accuracy: 48.08%\n",
      "Epoch #56 --> train_loss: 1.3713 | val_loss: 1.6899 | Validation Accuracy: 49.230000000000004%\n",
      "Epoch #57 --> train_loss: 1.4400 | val_loss: 1.2735 | Validation Accuracy: 49.16%\n",
      "Epoch #58 --> train_loss: 1.1811 | val_loss: 1.4102 | Validation Accuracy: 49.370000000000005%\n",
      "Epoch #59 --> train_loss: 1.0968 | val_loss: 1.7343 | Validation Accuracy: 49.28%\n",
      "Epoch #60 --> train_loss: 1.0876 | val_loss: 1.4507 | Validation Accuracy: 50.260000000000005%\n",
      "Epoch #61 --> train_loss: 1.2842 | val_loss: 1.5405 | Validation Accuracy: 50.31%\n",
      "Epoch #62 --> train_loss: 1.0744 | val_loss: 1.8067 | Validation Accuracy: 50.06%\n",
      "Epoch #63 --> train_loss: 1.3208 | val_loss: 1.7227 | Validation Accuracy: 48.91%\n",
      "Epoch #64 --> train_loss: 1.3068 | val_loss: 1.7630 | Validation Accuracy: 44.01%\n",
      "Epoch #65 --> train_loss: 1.1537 | val_loss: 1.7786 | Validation Accuracy: 49.980000000000004%\n",
      "Epoch #66 --> train_loss: 1.2103 | val_loss: 1.6838 | Validation Accuracy: 50.019999999999996%\n",
      "Epoch #67 --> train_loss: 1.2978 | val_loss: 1.3640 | Validation Accuracy: 48.870000000000005%\n",
      "Epoch #68 --> train_loss: 1.2137 | val_loss: 1.5821 | Validation Accuracy: 49.09%\n",
      "Epoch #69 --> train_loss: 1.1279 | val_loss: 2.0568 | Validation Accuracy: 50.11%\n",
      "Epoch #70 --> train_loss: 1.3859 | val_loss: 1.3155 | Validation Accuracy: 49.980000000000004%\n",
      "Epoch #71 --> train_loss: 1.4522 | val_loss: 1.6235 | Validation Accuracy: 48.36%\n",
      "Epoch #72 --> train_loss: 1.3426 | val_loss: 1.8555 | Validation Accuracy: 50.080000000000005%\n",
      "Epoch #73 --> train_loss: 1.3177 | val_loss: 1.9236 | Validation Accuracy: 47.71%\n",
      "Epoch #74 --> train_loss: 1.1874 | val_loss: 1.6582 | Validation Accuracy: 48.76%\n",
      "Epoch #75 --> train_loss: 1.3038 | val_loss: 1.7250 | Validation Accuracy: 48.5%\n",
      "Epoch #76 --> train_loss: 1.2061 | val_loss: 1.6785 | Validation Accuracy: 48.43%\n",
      "Epoch #77 --> train_loss: 1.1509 | val_loss: 1.5023 | Validation Accuracy: 49.93%\n",
      "Epoch #78 --> train_loss: 1.0241 | val_loss: 2.2022 | Validation Accuracy: 50.36000000000001%\n",
      "Epoch #79 --> train_loss: 1.1131 | val_loss: 1.5811 | Validation Accuracy: 49.88%\n",
      "Epoch #80 --> train_loss: 1.3694 | val_loss: 1.5531 | Validation Accuracy: 48.8%\n",
      "Epoch #81 --> train_loss: 1.2314 | val_loss: 1.8559 | Validation Accuracy: 49.94%\n",
      "Epoch #82 --> train_loss: 1.2173 | val_loss: 1.6144 | Validation Accuracy: 50.27%\n",
      "Epoch #83 --> train_loss: 1.1599 | val_loss: 1.4316 | Validation Accuracy: 48.84%\n",
      "Epoch #84 --> train_loss: 1.2810 | val_loss: 1.6443 | Validation Accuracy: 50.160000000000004%\n",
      "Epoch #85 --> train_loss: 1.1902 | val_loss: 1.6189 | Validation Accuracy: 49.5%\n",
      "Epoch #86 --> train_loss: 1.0384 | val_loss: 1.7746 | Validation Accuracy: 49.57%\n",
      "Epoch #87 --> train_loss: 0.9372 | val_loss: 1.7381 | Validation Accuracy: 50.39%\n",
      "Epoch #88 --> train_loss: 1.1183 | val_loss: 1.5467 | Validation Accuracy: 47.97%\n",
      "Epoch #89 --> train_loss: 1.4762 | val_loss: 1.8545 | Validation Accuracy: 49.21%\n",
      "Epoch #90 --> train_loss: 1.1933 | val_loss: 1.5005 | Validation Accuracy: 50.0%\n",
      "Epoch #91 --> train_loss: 1.2917 | val_loss: 1.6127 | Validation Accuracy: 48.92%\n",
      "Epoch #92 --> train_loss: 1.0549 | val_loss: 1.9510 | Validation Accuracy: 42.79%\n",
      "Epoch #93 --> train_loss: 1.0628 | val_loss: 1.6955 | Validation Accuracy: 49.71%\n",
      "Epoch #94 --> train_loss: 1.1354 | val_loss: 1.7834 | Validation Accuracy: 50.59%\n",
      "Epoch #95 --> train_loss: 1.1862 | val_loss: 1.7990 | Validation Accuracy: 49.61%\n",
      "Epoch #96 --> train_loss: 1.2430 | val_loss: 1.4022 | Validation Accuracy: 50.12%\n",
      "Epoch #97 --> train_loss: 0.9171 | val_loss: 1.4907 | Validation Accuracy: 49.88%\n",
      "Epoch #98 --> train_loss: 1.0425 | val_loss: 1.6584 | Validation Accuracy: 50.63999999999999%\n",
      "Epoch #99 --> train_loss: 1.4269 | val_loss: 1.6392 | Validation Accuracy: 48.559999999999995%\n",
      "Epoch #100 --> train_loss: 1.0878 | val_loss: 1.6215 | Validation Accuracy: 50.449999999999996%\n",
      "Epoch #101 --> train_loss: 1.3631 | val_loss: 1.5031 | Validation Accuracy: 48.99%\n",
      "Epoch #102 --> train_loss: 1.1086 | val_loss: 1.7540 | Validation Accuracy: 47.86%\n",
      "Epoch #103 --> train_loss: 0.9785 | val_loss: 1.8721 | Validation Accuracy: 49.68%\n",
      "Epoch #104 --> train_loss: 1.3510 | val_loss: 1.9091 | Validation Accuracy: 49.519999999999996%\n",
      "Epoch #105 --> train_loss: 1.1089 | val_loss: 1.8063 | Validation Accuracy: 49.36%\n",
      "Epoch #106 --> train_loss: 1.0144 | val_loss: 1.8888 | Validation Accuracy: 48.75%\n",
      "Epoch #107 --> train_loss: 0.9590 | val_loss: 1.8312 | Validation Accuracy: 50.239999999999995%\n",
      "Epoch #108 --> train_loss: 1.3498 | val_loss: 1.7584 | Validation Accuracy: 49.68%\n",
      "Epoch #109 --> train_loss: 0.9989 | val_loss: 1.5314 | Validation Accuracy: 50.19%\n",
      "Epoch #110 --> train_loss: 0.8901 | val_loss: 1.9683 | Validation Accuracy: 48.91%\n",
      "Epoch #111 --> train_loss: 1.0295 | val_loss: 1.8899 | Validation Accuracy: 49.7%\n",
      "Epoch #112 --> train_loss: 1.0300 | val_loss: 1.9270 | Validation Accuracy: 49.17%\n",
      "Epoch #113 --> train_loss: 0.9370 | val_loss: 1.6274 | Validation Accuracy: 50.129999999999995%\n",
      "Epoch #114 --> train_loss: 1.0146 | val_loss: 1.2909 | Validation Accuracy: 50.11%\n",
      "Epoch #115 --> train_loss: 1.1990 | val_loss: 1.7041 | Validation Accuracy: 49.59%\n",
      "Epoch #116 --> train_loss: 1.0226 | val_loss: 1.5629 | Validation Accuracy: 49.19%\n",
      "Epoch #117 --> train_loss: 1.1060 | val_loss: 1.6704 | Validation Accuracy: 49.72%\n",
      "Epoch #118 --> train_loss: 1.0499 | val_loss: 1.5950 | Validation Accuracy: 49.39%\n",
      "Epoch #119 --> train_loss: 0.9769 | val_loss: 1.5075 | Validation Accuracy: 49.86%\n",
      "Epoch #120 --> train_loss: 1.0940 | val_loss: 1.2492 | Validation Accuracy: 48.43%\n",
      "Epoch #121 --> train_loss: 1.0156 | val_loss: 1.6354 | Validation Accuracy: 49.65%\n",
      "Epoch #122 --> train_loss: 1.1043 | val_loss: 1.8142 | Validation Accuracy: 49.24%\n",
      "Epoch #123 --> train_loss: 1.1275 | val_loss: 1.7453 | Validation Accuracy: 50.07%\n",
      "Epoch #124 --> train_loss: 1.1832 | val_loss: 1.5833 | Validation Accuracy: 49.79%\n",
      "Epoch #125 --> train_loss: 1.0504 | val_loss: 1.7341 | Validation Accuracy: 48.41%\n",
      "Epoch #126 --> train_loss: 0.9580 | val_loss: 1.6979 | Validation Accuracy: 49.669999999999995%\n",
      "Epoch #127 --> train_loss: 0.8704 | val_loss: 1.9635 | Validation Accuracy: 43.94%\n",
      "Epoch #128 --> train_loss: 1.1488 | val_loss: 1.8326 | Validation Accuracy: 49.7%\n",
      "Epoch #129 --> train_loss: 0.9580 | val_loss: 1.9349 | Validation Accuracy: 49.34%\n",
      "Epoch #130 --> train_loss: 0.9357 | val_loss: 1.4405 | Validation Accuracy: 49.49%\n",
      "Epoch #131 --> train_loss: 0.9958 | val_loss: 1.7010 | Validation Accuracy: 49.49%\n",
      "Epoch #132 --> train_loss: 0.9588 | val_loss: 1.6169 | Validation Accuracy: 49.2%\n",
      "Epoch #133 --> train_loss: 0.9700 | val_loss: 1.8579 | Validation Accuracy: 49.919999999999995%\n",
      "Epoch #134 --> train_loss: 0.8937 | val_loss: 1.6645 | Validation Accuracy: 49.519999999999996%\n",
      "Epoch #135 --> train_loss: 0.8696 | val_loss: 1.6440 | Validation Accuracy: 49.24%\n",
      "Epoch #136 --> train_loss: 0.9804 | val_loss: 1.7987 | Validation Accuracy: 49.89%\n",
      "Epoch #137 --> train_loss: 0.9222 | val_loss: 1.9866 | Validation Accuracy: 49.41%\n",
      "Epoch #138 --> train_loss: 0.8559 | val_loss: 1.5502 | Validation Accuracy: 48.559999999999995%\n",
      "Epoch #139 --> train_loss: 0.8698 | val_loss: 1.6891 | Validation Accuracy: 49.61%\n",
      "Epoch #140 --> train_loss: 0.9363 | val_loss: 1.6323 | Validation Accuracy: 49.44%\n",
      "Epoch #141 --> train_loss: 1.0928 | val_loss: 1.7252 | Validation Accuracy: 48.54%\n",
      "Epoch #142 --> train_loss: 1.1085 | val_loss: 1.5666 | Validation Accuracy: 48.93%\n",
      "Epoch #143 --> train_loss: 0.8714 | val_loss: 2.1354 | Validation Accuracy: 48.089999999999996%\n",
      "Epoch #144 --> train_loss: 0.9302 | val_loss: 1.6760 | Validation Accuracy: 48.980000000000004%\n",
      "Epoch #145 --> train_loss: 0.9493 | val_loss: 1.8986 | Validation Accuracy: 49.26%\n",
      "Epoch #146 --> train_loss: 0.9803 | val_loss: 1.7669 | Validation Accuracy: 49.19%\n",
      "Epoch #147 --> train_loss: 0.8320 | val_loss: 1.7634 | Validation Accuracy: 49.4%\n",
      "Epoch #148 --> train_loss: 0.9638 | val_loss: 1.6102 | Validation Accuracy: 48.66%\n",
      "Epoch #149 --> train_loss: 1.0131 | val_loss: 2.0189 | Validation Accuracy: 48.82%\n",
      "Epoch #150 --> train_loss: 1.0241 | val_loss: 1.9233 | Validation Accuracy: 47.57%\n",
      "Epoch #151 --> train_loss: 0.9549 | val_loss: 1.9958 | Validation Accuracy: 49.54%\n",
      "Epoch #152 --> train_loss: 0.9173 | val_loss: 1.8328 | Validation Accuracy: 48.25%\n",
      "Epoch #153 --> train_loss: 1.1935 | val_loss: 1.5823 | Validation Accuracy: 49.27%\n",
      "Epoch #154 --> train_loss: 1.0470 | val_loss: 1.8159 | Validation Accuracy: 49.33%\n",
      "Epoch #155 --> train_loss: 0.9617 | val_loss: 1.6196 | Validation Accuracy: 49.519999999999996%\n",
      "Epoch #156 --> train_loss: 1.1200 | val_loss: 1.6869 | Validation Accuracy: 49.3%\n",
      "Epoch #157 --> train_loss: 1.0061 | val_loss: 1.7063 | Validation Accuracy: 49.16%\n",
      "Epoch #158 --> train_loss: 0.9543 | val_loss: 1.9895 | Validation Accuracy: 49.38%\n",
      "Epoch #159 --> train_loss: 0.9420 | val_loss: 1.7695 | Validation Accuracy: 49.230000000000004%\n",
      "Epoch #160 --> train_loss: 1.0164 | val_loss: 1.6087 | Validation Accuracy: 48.9%\n",
      "Epoch #161 --> train_loss: 0.8344 | val_loss: 1.6589 | Validation Accuracy: 48.199999999999996%\n",
      "Epoch #162 --> train_loss: 0.9015 | val_loss: 1.3739 | Validation Accuracy: 47.81%\n",
      "Epoch #163 --> train_loss: 0.9617 | val_loss: 1.8381 | Validation Accuracy: 49.21%\n",
      "Epoch #164 --> train_loss: 0.8458 | val_loss: 2.1657 | Validation Accuracy: 49.059999999999995%\n",
      "Epoch #165 --> train_loss: 0.8196 | val_loss: 1.8124 | Validation Accuracy: 48.77%\n",
      "Epoch #166 --> train_loss: 1.0667 | val_loss: 2.0733 | Validation Accuracy: 48.65%\n",
      "Epoch #167 --> train_loss: 0.9731 | val_loss: 1.6917 | Validation Accuracy: 49.09%\n",
      "Epoch #168 --> train_loss: 0.8935 | val_loss: 2.2241 | Validation Accuracy: 49.14%\n",
      "Epoch #169 --> train_loss: 0.9412 | val_loss: 1.4838 | Validation Accuracy: 49.13%\n",
      "Epoch #170 --> train_loss: 1.0727 | val_loss: 2.0531 | Validation Accuracy: 49.3%\n",
      "Epoch #171 --> train_loss: 1.0299 | val_loss: 1.5505 | Validation Accuracy: 48.76%\n",
      "Epoch #172 --> train_loss: 0.9036 | val_loss: 1.9764 | Validation Accuracy: 49.05%\n",
      "Epoch #173 --> train_loss: 0.8155 | val_loss: 1.7750 | Validation Accuracy: 49.01%\n",
      "Epoch #174 --> train_loss: 0.8075 | val_loss: 2.1907 | Validation Accuracy: 49.16%\n",
      "Epoch #175 --> train_loss: 0.8773 | val_loss: 1.9542 | Validation Accuracy: 47.3%\n",
      "Epoch #176 --> train_loss: 0.9663 | val_loss: 1.8247 | Validation Accuracy: 49.01%\n",
      "Epoch #177 --> train_loss: 0.8308 | val_loss: 1.9059 | Validation Accuracy: 48.99%\n",
      "Epoch #178 --> train_loss: 0.9937 | val_loss: 1.3804 | Validation Accuracy: 49.16%\n",
      "Epoch #179 --> train_loss: 0.8090 | val_loss: 1.9303 | Validation Accuracy: 49.03%\n",
      "Epoch #180 --> train_loss: 0.9017 | val_loss: 1.7216 | Validation Accuracy: 49.44%\n",
      "Epoch #181 --> train_loss: 0.9331 | val_loss: 1.9139 | Validation Accuracy: 48.9%\n",
      "Epoch #182 --> train_loss: 1.0246 | val_loss: 2.0432 | Validation Accuracy: 46.89%\n",
      "Epoch #183 --> train_loss: 0.8061 | val_loss: 1.7230 | Validation Accuracy: 49.2%\n",
      "Epoch #184 --> train_loss: 0.8716 | val_loss: 1.4415 | Validation Accuracy: 49.32%\n",
      "Epoch #185 --> train_loss: 0.8275 | val_loss: 1.8403 | Validation Accuracy: 49.29%\n",
      "Epoch #186 --> train_loss: 0.8258 | val_loss: 1.6680 | Validation Accuracy: 49.059999999999995%\n",
      "Epoch #187 --> train_loss: 0.8142 | val_loss: 1.1418 | Validation Accuracy: 48.28%\n",
      "Epoch #188 --> train_loss: 0.9296 | val_loss: 2.0546 | Validation Accuracy: 49.0%\n",
      "Epoch #189 --> train_loss: 0.8376 | val_loss: 2.1917 | Validation Accuracy: 49.11%\n",
      "Epoch #190 --> train_loss: 0.9359 | val_loss: 1.6552 | Validation Accuracy: 48.91%\n",
      "Epoch #191 --> train_loss: 0.9397 | val_loss: 2.0092 | Validation Accuracy: 49.47%\n",
      "Epoch #192 --> train_loss: 1.0547 | val_loss: 1.2453 | Validation Accuracy: 49.02%\n",
      "Epoch #193 --> train_loss: 0.8430 | val_loss: 1.8632 | Validation Accuracy: 49.34%\n",
      "Epoch #194 --> train_loss: 0.9172 | val_loss: 1.7947 | Validation Accuracy: 49.309999999999995%\n",
      "Epoch #195 --> train_loss: 0.8279 | val_loss: 2.0328 | Validation Accuracy: 48.85%\n",
      "Epoch #196 --> train_loss: 1.1355 | val_loss: 1.6781 | Validation Accuracy: 48.949999999999996%\n",
      "Epoch #197 --> train_loss: 0.9221 | val_loss: 1.5259 | Validation Accuracy: 48.88%\n",
      "Epoch #198 --> train_loss: 0.8521 | val_loss: 1.7691 | Validation Accuracy: 48.809999999999995%\n",
      "Epoch #199 --> train_loss: 1.2144 | val_loss: 1.6716 | Validation Accuracy: 49.120000000000005%\n"
     ]
    }
   ],
   "source": [
    "net = Net2()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "optimizer.zero_grad()\n",
    "for epoch in range(200):\n",
    "    loss = 0\n",
    "    val_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    Acc = 0\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = net(imgs.view(batch_size, -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            batch_size = imgs.shape[0]\n",
    "            outputs = outputs = net(imgs.view(batch_size, -1))\n",
    "            val_loss = loss_fn(outputs, labels)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "            Acc = (correct / total)*100\n",
    "    print(f\"Epoch #{epoch} --> train_loss: {loss.item():.4f} | val_loss: {val_loss.item():.4f} | Validation Accuracy: {Acc}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module Net is treated as a zero-op.\n",
      "Net(\n",
      "  1.579 M, 100.000% Params, 0.002 GMac, 100.000% MACs, \n",
      "  (hidden): Linear(1.573 M, 99.675% Params, 0.002 GMac, 99.675% MACs, in_features=3072, out_features=512, bias=True)\n",
      "  (predict): Linear(0.005 M, 0.325% Params, 0.0 GMac, 0.325% MACs, in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Computational complexity:       0.0 GMac\n",
      "Number of parameters:           1.58 M  \n",
      "Warning: module Net2 is treated as a zero-op.\n",
      "Net2(\n",
      "  1.643 M, 100.000% Params, 0.002 GMac, 100.000% MACs, \n",
      "  (hidden1): Linear(1.573 M, 95.733% Params, 0.002 GMac, 95.733% MACs, in_features=3072, out_features=512, bias=True)\n",
      "  (hidden2): Linear(0.066 M, 3.995% Params, 0.0 GMac, 3.995% MACs, in_features=512, out_features=128, bias=True)\n",
      "  (hidden3): Linear(0.004 M, 0.251% Params, 0.0 GMac, 0.251% MACs, in_features=128, out_features=32, bias=True)\n",
      "  (predict): Linear(0.0 M, 0.020% Params, 0.0 GMac, 0.020% MACs, in_features=32, out_features=10, bias=True)\n",
      ")\n",
      "Computational complexity:       0.0 GMac\n",
      "Number of parameters:           1.64 M  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/armin/.local/lib/python3.8/site-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "net_test = Net()\n",
    "net2_test = Net2()\n",
    "macs, params = get_model_complexity_info(net_test, (1, 3*32*32), \n",
    "                                           as_strings=True,\n",
    "                                           print_per_layer_stat=True, \n",
    "                                           verbose=True)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "macs, params = get_model_complexity_info(net2_test, (1, 3*32*32), \n",
    "                                           as_strings=True,\n",
    "                                           print_per_layer_stat=True, \n",
    "                                           verbose=True)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
